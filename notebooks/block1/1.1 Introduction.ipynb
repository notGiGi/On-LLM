{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff9d60c",
   "metadata": {},
   "source": [
    "# Block 1 — Mathematical Foundations of Tokenization, Batching & Attention Masking\n",
    "\n",
    "This notebook builds the **mathematical backbone** for how LLMs go from raw text to tokens, how batches are formed and losses computed, and how attention masks (causal/padding/head) enforce the same principles architecturally.\n",
    "\n",
    "---\n",
    "\n",
    "## What you will learn in Block 1\n",
    "1. **Objects & notation**: strings, normalization, tokenizer, vocabulary, special tokens.  \n",
    "2. **Autoregressive factorization** of sequence probability.  \n",
    "3. **Maximum-likelihood objective (NLL)** and its link to **cross-entropy**.  \n",
    "4. **Batching with padding** and the **masked mean token loss**.  \n",
    "5. **Perplexity** as exponential of mean NLL.  \n",
    "6. **Attention masking (preview)**: causal mask, key-padding mask, and head masking.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Objects and notation\n",
    "\n",
    "- **Alphabet & strings.**  \n",
    "  Let $\\Sigma$ be the raw character alphabet (e.g., Unicode code points).  \n",
    "  Any finite string is an element:\n",
    "  $$\n",
    "  x \\in \\Sigma^{*}.\n",
    "  $$\n",
    "\n",
    "- **Normalization.**  \n",
    "  A preprocessing map:\n",
    "  $$\n",
    "  \\nu:\\Sigma^{*} \\to \\Sigma^{*}, \\qquad x \\mapsto \\nu(x),\n",
    "  $$\n",
    "  that may include:\n",
    "  - Unicode normalization (e.g., NFKC),  \n",
    "  - case folding (lowercasing),  \n",
    "  - whitespace and punctuation rules.  \n",
    "\n",
    "  *Remark:* Normalization **changes the support** of the data distribution by altering how strings map to tokens (e.g., “Café” → “Cafe”).\n",
    "\n",
    "- **Tokenizer.**  \n",
    "  A **tokenizer** is a function that segments normalized text into discrete units (tokens) from a fixed vocabulary $\\mathcal{V}$.  \n",
    "  Mathematically:\n",
    "  $$\n",
    "  \\tau: \\Sigma^{*} \\to \\mathcal{V}^{*}, \\qquad\n",
    "  \\mathbf{t}=\\tau(\\nu(x))=(t_1,\\dots,t_{L(x)}), \\quad t_i \\in \\mathcal{V}.\n",
    "  $$\n",
    "\n",
    "  \n",
    "  - Tokenizers define the *basic symbols* LLMs see.  \n",
    "  - They can split text into **characters** (character-level), **words** (word-level), or **subwords** (BPE/Unigram).  \n",
    "  - Subword tokenization (e.g., “tokenization” → `[\"token\", \"ization\"]`) balances vocabulary size with sequence length.  \n",
    "  - This mapping is deterministic: the same string always yields the same token sequence.  \n",
    "  - Without tokenization, raw text would be unmanageable: the space of possible strings $\\Sigma^{*}$ is infinite, but the space of tokens $\\mathcal{V}$ is finite and tractable.\n",
    "\n",
    "- **Vocabulary & special tokens.**  \n",
    "  The vocabulary $\\mathcal{V}=\\{0,1,\\dots,V-1\\}$ contains learned tokens plus reserved IDs:\n",
    "  - `<bos>` : beginning of sequence  \n",
    "  - `<eos>` : end of sequence  \n",
    "  - `<pad>` : padding (used in batching)  \n",
    "  - `<unk>` : unknown token (for OOV strings)\n",
    "\n",
    "- **Augmented sequence (optional).**  \n",
    "  To mark boundaries explicitly:\n",
    "  $$\n",
    "  \\tilde{\\mathbf{t}} = (\\texttt{<bos>},\\, t_1,\\dots,t_{L(x)},\\, \\texttt{<eos>})\n",
    "  $$\n",
    "\n",
    "- **Dataset.**  \n",
    "  A dataset is a collection of $N$ independent samples:\n",
    "  $$\n",
    "  \\mathcal{D} = \\{\\, x^{(n)} \\,\\}_{n=1}^{N}.\n",
    "  $$\n",
    "  After tokenization:\n",
    "  $$\n",
    "  \\mathcal{D}_{\\text{tok}} = \\{\\, \\mathbf{t}^{(n)} \\,\\}_{n=1}^{N}, \\qquad\n",
    "  \\mathbf{t}^{(n)} = (t^{(n)}_1, \\dots, t^{(n)}_{L(x^{(n)})}).\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Autoregressive factorization (preview)\n",
    "\n",
    "An LLM with parameters $\\theta$ assigns probability to a tokenized string via the **causal chain rule**:\n",
    "\n",
    "$$\n",
    "p_\\theta(x) = \\prod_{i=1}^{L(x)} p_\\theta\\!\\big(t_i \\mid t_{<i}\\big),\n",
    "\\qquad t_{<i}=(t_1,\\dots,t_{i-1}).\n",
    "$$\n",
    "\n",
    "With explicit boundaries:\n",
    "$$\n",
    "p_\\theta(x) = \\prod_{i=1}^{\\tilde L} p_\\theta\\!\\big(\\tilde t_i \\mid \\tilde t_{<i}\\big),\n",
    "\\quad \\tilde t_1=\\texttt{<bos>},\\ \\tilde t_{\\tilde L}=\\texttt{<eos>}.\n",
    "$$\n",
    "\n",
    "*Interpretation:*  \n",
    "The model decomposes the text into **conditional next-token probabilities**; causality forbids peeking at the future.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training objective (preview)\n",
    "\n",
    "Given dataset $\\mathcal{D}$, **maximum likelihood** maximizes the log-probability assigned to the data:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta}\\; \\sum_{x \\in \\mathcal{D}} \\sum_{i=1}^{L(x)} \\log p_\\theta\\!\\big(t_i \\mid t_{<i}\\big).\n",
    "$$\n",
    "\n",
    "Equivalently, we **minimize the negative log-likelihood (NLL)**.  \n",
    "Later we will connect this to **softmax cross-entropy**, show the **masked mean over valid tokens** (excluding `<pad>`), and define **perplexity**:\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\!\\big(\\text{mean NLL per valid token}\\big).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Attention masking (preview)\n",
    "\n",
    "Self-attention computes scaled dot products between queries and keys.  \n",
    "To respect the mathematics above, we add:\n",
    "- a **causal mask** (no attending to future positions),  \n",
    "- a **key-padding mask** (ignore `<pad>` positions),  \n",
    "- and optionally a **head mask** (enable/disable heads for ablation/interpretability).\n",
    "\n",
    "Later we will show how these masks combine into the attention logits **before** the softmax.\n",
    "\n",
    "---\n",
    "\n",
    "### Outcome of Block 1\n",
    "\n",
    "By the end of Block 1 you will be able to:\n",
    "- Map raw strings $\\to$ tokens with clear assumptions about normalization and special tokens.  \n",
    "- Write and reason about $p_\\theta(x)$ via autoregressive factorization.  \n",
    "- Derive the MLE/NLL objective and relate it to cross-entropy.  \n",
    "- Implement padding-aware **masked mean** losses and compute **perplexity**.  \n",
    "- Explain how **attention masks** (causal/padding/head) enforce the same constraints architecturally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bedfd1",
   "metadata": {},
   "source": [
    "## 2) Autoregressive Factorization and Training Objective\n",
    "\n",
    "Now that we understand what **tokens** are and how a **tokenizer** converts text into discrete units, we ask:  \n",
    "**How does a Large Language Model assign a probability to a whole sequence of tokens?**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 The chain rule of probability\n",
    "\n",
    "From basic probability theory:  \n",
    "For any sequence of random variables $(X_1, X_2, \\dots, X_L)$ we can always write:\n",
    "\n",
    "$$\n",
    "p(X_1, X_2, \\dots, X_L) = \\prod_{i=1}^L p(X_i \\mid X_1, \\dots, X_{i-1}).\n",
    "$$\n",
    "\n",
    "This is called the **chain rule**.  \n",
    "It is an identity, not an assumption — it always holds.\n",
    "\n",
    "- $p(X_1)$ is the probability of the first element.  \n",
    "- $p(X_2 \\mid X_1)$ is the probability of the second given the first.  \n",
    "- $p(X_3 \\mid X_1, X_2)$ is the probability of the third given the first two.  \n",
    "- And so on.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Applying the chain rule to tokens\n",
    "\n",
    "For a token sequence $\\mathbf{t} = (t_1, t_2, \\dots, t_L)$ produced by the tokenizer:\n",
    "\n",
    "$$\n",
    "p_\\theta(\\mathbf{t}) = \\prod_{i=1}^L p_\\theta(t_i \\mid t_1, \\dots, t_{i-1}),\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the model parameters (the weights of the neural network).\n",
    "\n",
    "- $t_i$ is the $i$-th token.  \n",
    "- $t_{<i} = (t_1, \\dots, t_{i-1})$ is the prefix (all tokens before $i$).  \n",
    "- Each factor is the probability of the **next token** given all previous ones.\n",
    "\n",
    "With explicit boundary tokens `<bos>` (begin) and `<eos>` (end):\n",
    "\n",
    "$$\n",
    "p_\\theta(\\tilde{\\mathbf{t}}) = \\prod_{i=1}^{\\tilde L} p_\\theta(\\tilde t_i \\mid \\tilde t_{<i}),\n",
    "$$\n",
    "\n",
    "where $\\tilde L = L + 2$ because of the added boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Why autoregression?\n",
    "\n",
    "- **Causality.** Humans generate text left-to-right; the model imitates this by only looking at the past.  \n",
    "- **Simplicity.** Instead of modeling an entire sequence directly, we only need to model *next-token prediction*.  \n",
    "- **Flexibility.** With this formulation we can generate text token by token: sample $t_1$ from $p(t_1)$, then $t_2$ from $p(t_2\\mid t_1)$, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Training objective: Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "We want our model to assign **high probability** to real text sequences from the dataset.  \n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{ \\mathbf{t}^{(n)} \\}_{n=1}^N$, the **log-likelihood** is:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathcal{D}) = \\sum_{n=1}^N \\log p_\\theta(\\mathbf{t}^{(n)}).\n",
    "$$\n",
    "\n",
    "Using the autoregressive factorization:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathbf{t}^{(n)}) = \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}).\n",
    "$$\n",
    "\n",
    "Thus the training objective is:\n",
    "\n",
    "$$\n",
    "\\max_\\theta \\; \\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta\\!\\big(t^{(n)}_i \\mid t^{(n)}_{<i}\\big).\n",
    "$$\n",
    "\n",
    "This is called **Maximum Likelihood Estimation (MLE).**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 Why logs?\n",
    "\n",
    "If we worked directly with the product $\\prod p(t_i \\mid t_{<i})$, probabilities quickly become astronomically small (multiplying many numbers less than 1).  \n",
    "Taking the logarithm:\n",
    "\n",
    "- Turns products into sums (easier to compute).  \n",
    "- Stabilizes numerics (avoids underflow).  \n",
    "- Matches information-theoretic interpretations (log-loss).  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Negative log-likelihood (NLL)\n",
    "\n",
    "Instead of maximizing log-likelihood, we minimize the **negative log-likelihood**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\theta) = -\\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}).\n",
    "$$\n",
    "\n",
    "This is the **loss function** of LLMs.  \n",
    "Minimizing it means: *the model gets penalized whenever it assigns low probability to the correct next token.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.7 Connection to cross-entropy\n",
    "\n",
    "At each position $i$, the model outputs a probability distribution over the vocabulary $\\mathcal{V}$.  \n",
    "\n",
    "If the true token is $t_i$, the per-token loss is:\n",
    "\n",
    "$$\n",
    "\\ell_i = -\\log p_\\theta(t_i \\mid t_{<i}).\n",
    "$$\n",
    "\n",
    "This is exactly the **cross-entropy** between the predicted distribution $p_\\theta(\\cdot \\mid t_{<i})$ and the true one-hot distribution $y$ where $y_{k} = 1$ if $k=t_i$:\n",
    "\n",
    "$$\n",
    "\\ell_i = H(y, p_\\theta) = - \\sum_{k=1}^{V} y_k \\log p_\\theta(k \\mid t_{<i}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.8 Gradient identity (intuition for learning)\n",
    "\n",
    "The derivative of the cross-entropy with respect to the logits $z_{i,k}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_{i,k}} = p_\\theta(k \\mid t_{<i}) - y_k.\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- If the model assigns too much probability to the wrong token, the gradient pushes it down.  \n",
    "- If the model assigns too little to the correct token, the gradient pushes it up.  \n",
    "- Learning is simply **adjusting logits so that predicted probabilities match observed tokens.**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.9 Intuition\n",
    "\n",
    "- Training an LLM = **teaching it to be a good next-token predictor**.  \n",
    "- By predicting the next token well across billions of examples, the model learns grammar, semantics, style, reasoning patterns, and world knowledge.  \n",
    "- All of this comes from one simple principle: *minimize negative log-likelihood (maximize the probability of real text).*\n",
    "\n",
    "---\n",
    "\n",
    "**Next (Block 1.3):**  \n",
    "We extend this objective to **batches of sequences** (with different lengths), introduce **padding**, and show how to compute a **masked mean token loss** that ignores `<pad>` tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23429d",
   "metadata": {},
   "source": [
    "## 3) Batching, Padding, and the Masked Token Loss\n",
    "\n",
    "So far we looked at single sequences. But in practice, models are trained with **batches** of sequences in parallel, for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Why batching?\n",
    "\n",
    "- GPUs/TPUs can process many examples at once.  \n",
    "- Instead of updating weights after every single sequence, we group **B sequences** into a **batch**.  \n",
    "- This allows **vectorized operations** and faster convergence (by averaging gradients).\n",
    "\n",
    "Formally:  \n",
    "A batch $\\mathcal{B}$ is a set of $B$ token sequences:\n",
    "$$\n",
    "\\mathcal{B} = \\{\\mathbf{t}^{(1)}, \\mathbf{t}^{(2)}, \\dots, \\mathbf{t}^{(B)}\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 The problem of variable lengths\n",
    "\n",
    "Different sequences have different lengths:\n",
    "- Example: `\"The cat\"` → 2 tokens.  \n",
    "- Example: `\"A very long sentence here\"` → 6 tokens.  \n",
    "\n",
    "Neural networks expect tensors with the **same shape**, so we cannot stack them directly.  \n",
    "Solution: **Padding**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Padding sequences\n",
    "\n",
    "Let $L_{\\max}$ be the length of the longest sequence in the batch.  \n",
    "We build a matrix:\n",
    "\n",
    "$$\n",
    "T \\in \\mathbb{N}^{B \\times L_{\\max}}, \\quad\n",
    "T_{b,i} =\n",
    "\\begin{cases}\n",
    "t^{(b)}_i & \\text{if } i \\leq L(\\mathbf{t}^{(b)}), \\\\\\\\\n",
    "\\texttt{<pad>} & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Each row = one sequence.  \n",
    "- Shorter sequences are padded with a special `<pad>` token until they reach length $L_{\\max}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Mask of valid tokens\n",
    "\n",
    "To ensure padding does not affect the loss, we build a **mask**:\n",
    "\n",
    "$$\n",
    "M \\in \\{0,1\\}^{B \\times L_{\\max}}, \\qquad\n",
    "M_{b,i} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } T_{b,i} \\neq \\texttt{<pad>} \\quad (\\text{valid token}), \\\\\\\\\n",
    "0 & \\text{if } T_{b,i} = \\texttt{<pad>} \\quad (\\text{ignored}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Masked mean token loss\n",
    "\n",
    "Without masking, the loss would include `<pad>` tokens, corrupting training.  \n",
    "Instead we average only over **valid tokens**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{token}} =\n",
    "- \\frac{1}{N_{\\text{valid}}}\n",
    "\\sum_{b=1}^B \\sum_{i=1}^{L_{\\max}}\n",
    "M_{b,i} \\; \\log p_\\theta\\!\\big(T_{b,i} \\mid T_{b,<i}\\big),\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "N_{\\text{valid}} = \\sum_{b=1}^B \\sum_{i=1}^{L_{\\max}} M_{b,i}.\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "- Loss is **independent of how much padding there is**.  \n",
    "- Gradients reflect only *real tokens*.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.6 Implementation intuition\n",
    "\n",
    "Most deep learning frameworks implement this using:\n",
    "- A **CrossEntropyLoss** with `ignore_index=pad_id` (in PyTorch).  \n",
    "- An **attention_mask** to indicate valid tokens (in Transformers).\n",
    "\n",
    "Thus:\n",
    "- Forward pass uses the padded tensor.  \n",
    "- Loss computation ignores `<pad>`.  \n",
    "- Attention layers also ignore `<pad>` (via masking).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600f8717",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Suppose we have 3 sequences of different lengths\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose we have 3 sequences of different lengths\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],      # length 4\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id], # length 3\n",
    "    [12, 13, 14, 15, 16, 17]            # length 6 (max)\n",
    "])\n",
    "\n",
    "print(\"Batch shape:\", batch.shape)  # (3, 6)\n",
    "\n",
    "# Mask: 1 for valid tokens, 0 for pad\n",
    "mask = (batch != pad_id).long()\n",
    "print(\"Mask:\\n\", mask)\n",
    "\n",
    "# Example: logits from a model (random for demo)\n",
    "B, L, V = batch.shape[0], batch.shape[1], 20  # vocab size = 20\n",
    "logits = torch.randn(B, L, V)  # model output\n",
    "\n",
    "# Loss function that ignores pad tokens\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "# Shift targets by one position for next-token prediction\n",
    "targets = batch.clone()\n",
    "targets[targets == pad_id] = pad_id  # keep pad as pad\n",
    "print(\"Targets:\\n\", targets)\n",
    "\n",
    "# Flatten for CE loss\n",
    "loss = loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "print(\"Masked loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7eba7d",
   "metadata": {},
   "source": [
    "## 4) Perplexity: measuring how well a model predicts\n",
    "\n",
    "Up to now we focused on the **loss** (negative log-likelihood). But in language modeling, people often report **perplexity (PPL)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Definition\n",
    "\n",
    "For a dataset of $N_{\\text{valid}}$ valid tokens, perplexity is:\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\!\\Bigg( \\frac{1}{N_{\\text{valid}}}\n",
    "\\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})}\n",
    "-\\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}) \\Bigg).\n",
    "$$\n",
    "\n",
    "This is simply the exponential of the **average negative log-likelihood per token**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Intuition\n",
    "\n",
    "- If $\\mathrm{PPL}=1$: the model is perfect — it always assigns probability 1 to the correct token.  \n",
    "- If $\\mathrm{PPL}=V$ (vocabulary size): the model is as bad as random guessing.  \n",
    "- Lower perplexity = better model.\n",
    "\n",
    "In words: **perplexity measures “how many tokens the model is confused among, on average.”**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Why use perplexity?\n",
    "\n",
    "- It is easier to interpret than raw log-loss.  \n",
    "- It is standard in language modeling benchmarks (e.g., Penn Treebank, WikiText).  \n",
    "- It allows fair comparison across models trained with different vocabularies (to some extent).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Example with code\n",
    "Now let’s compute perplexity from the loss in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Suppose we have predictions for a batch\n",
    "pad_id = 0\n",
    "targets = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id],\n",
    "    [12, 13, 14, 15, 16, 17]\n",
    "])\n",
    "\n",
    "B, L = targets.shape\n",
    "V = 20\n",
    "logits = torch.randn(B, L, V)\n",
    "\n",
    "# Loss function that ignores padding\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id, reduction=\"sum\")\n",
    "\n",
    "# Compute total loss (sum over tokens, not mean yet)\n",
    "loss_sum = loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "\n",
    "# Count valid tokens (non-pad)\n",
    "valid_tokens = (targets != pad_id).sum().item()\n",
    "\n",
    "# Average NLL per token\n",
    "nll_avg = loss_sum.item() / valid_tokens\n",
    "\n",
    "# Perplexity\n",
    "ppl = math.exp(nll_avg)\n",
    "\n",
    "print(\"Average NLL per token:\", nll_avg)\n",
    "print(\"Perplexity:\", ppl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6e9b6",
   "metadata": {},
   "source": [
    "## 5) Attention Masking: Causal, Padding, and Head Masks\n",
    "\n",
    "So far we focused on **token probabilities** and **loss functions**.  \n",
    "Now we connect this theory to the **attention mechanism** inside Transformers — the core building block of LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 Self-attention in a nutshell\n",
    "\n",
    "In self-attention, every token can attend to every other token.  \n",
    "For a sequence of length $L$ and embedding size $d$, we compute:\n",
    "\n",
    "- **Queries:** $Q \\in \\mathbb{R}^{L \\times d}$  \n",
    "- **Keys:** $K \\in \\mathbb{R}^{L \\times d}$  \n",
    "- **Values:** $V \\in \\mathbb{R}^{L \\times d}$  \n",
    "\n",
    "The **attention scores** are:\n",
    "\n",
    "$$\n",
    "S = \\frac{QK^\\top}{\\sqrt{d}} \\quad \\in \\mathbb{R}^{L \\times L}.\n",
    "$$\n",
    "\n",
    "After applying softmax row-wise, we get the **attention weights**:\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}(S),\n",
    "$$\n",
    "\n",
    "and then the output is:\n",
    "\n",
    "$$\n",
    "O = A \\, V.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 The problem\n",
    "\n",
    "- Without restrictions, a token at position $i$ could attend to **future tokens** $j>i$.  \n",
    "- `<pad>` tokens could receive attention, contaminating representations.  \n",
    "- Sometimes we want to **disable certain heads** for ablation or interpretability.\n",
    "\n",
    "**Solution:** use **masks** that tell the model where it is allowed to attend.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Causal mask (autoregression)\n",
    "\n",
    "**Goal:** prevent a token from looking into the future.\n",
    "\n",
    "Definition:\n",
    "\n",
    "$$\n",
    "C \\in \\{0,1\\}^{L \\times L}, \\qquad\n",
    "C_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } j>i \\quad (\\text{future blocked}), \\\\\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Applied to scores:\n",
    "\n",
    "$$\n",
    "S \\;\\leftarrow\\; S + (-10^9)\\cdot C.\n",
    "$$\n",
    "\n",
    "This sets future positions to a very large negative value (≈$-\\infty$), so after softmax they get probability 0.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Padding mask\n",
    "\n",
    "**Goal:** ignore `<pad>` tokens added during batching.\n",
    "\n",
    "Definition for a batch of size $B$:\n",
    "\n",
    "$$\n",
    "P \\in \\{0,1\\}^{B \\times L}, \\qquad\n",
    "P_{b,j} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if token $j$ in sequence $b$ is <pad>}, \\\\\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "When broadcasted into attention scores, positions marked with 1 are also masked out.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Head mask\n",
    "\n",
    "**Goal:** enable or disable specific heads.\n",
    "\n",
    "For $H$ attention heads:\n",
    "\n",
    "$$\n",
    "h \\in \\{0,1\\}^H \\quad \\text{(or } h \\in \\{0,1\\}^{B \\times H}\\text{ if per-example)}.\n",
    "$$\n",
    "\n",
    "- $h_h=0$ → that head is disabled (all its contributions zeroed).  \n",
    "- Useful for **interpretability** (probing which heads matter), or for **structured pruning**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.6 Combined masking\n",
    "\n",
    "For a full batch, the final scores are:\n",
    "\n",
    "$$\n",
    "S = \\frac{QK^\\top}{\\sqrt{d}}.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Broadcast $C$ (causal mask) to $(1,1,L,L)$.  \n",
    "2. Broadcast $P$ (padding mask) to $(B,1,1,L)$.  \n",
    "3. Combine:  \n",
    "   $$\n",
    "   M = \\text{broadcast}(C) \\;\\lor\\; \\text{broadcast}(P).\n",
    "   $$\n",
    "4. Apply to scores:  \n",
    "   $$\n",
    "   S \\;\\leftarrow\\; S + (-10^9)\\cdot M.\n",
    "   $$\n",
    "5. Softmax row-wise over keys:  \n",
    "   $$\n",
    "   A = \\text{softmax}(S).\n",
    "   $$\n",
    "6. Apply head mask (if any):  \n",
    "   $$\n",
    "   A \\;\\leftarrow\\; A \\odot \\text{broadcast}(h).\n",
    "   $$\n",
    "7. Final output:  \n",
    "   $$\n",
    "   O = A \\, V.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.7 Why attention masking is essential\n",
    "\n",
    "- **Causal mask:** enforces the autoregressive factorization (no cheating by looking at the future).  \n",
    "- **Padding mask:** ensures that `<pad>` tokens do not leak into the computation.  \n",
    "- **Head mask:** gives flexibility for analysis and pruning.\n",
    "\n",
    "Together, these masks align the **mathematics of training (loss + padding)** with the **architecture of attention**.\n",
    "\n",
    "---\n",
    "\n",
    "**Next (Block 1.6):**  \n",
    "We will implement these masks in **PyTorch code**:  \n",
    "1. Build a causal mask matrix.  \n",
    "2. Build a padding mask from a batch with `<pad>`.  \n",
    "3. Show how they are applied inside a MultiheadAttention layer.  \n",
    "4. Visualize the causal mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ff7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Create toy batch (with padding)\n",
    "# -------------------------------\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],      # length 4\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id], # length 3\n",
    "    [12, 13, 14, 15, 16, 17]            # length 6 (max)\n",
    "])\n",
    "\n",
    "B, L = batch.shape\n",
    "D = 16   # embedding dimension\n",
    "H = 2    # number of heads\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build causal mask\n",
    "# -------------------------------\n",
    "# Shape: (L, L)\n",
    "causal_mask = torch.triu(torch.ones(L, L) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "print(\"Causal mask:\\n\", causal_mask)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Build key-padding mask\n",
    "# -------------------------------\n",
    "# True where PAD tokens are present\n",
    "padding_mask = (batch == pad_id)\n",
    "print(\"Padding mask shape:\", padding_mask.shape)  # (B, L)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. MultiheadAttention with masks\n",
    "# -------------------------------\n",
    "attn = nn.MultiheadAttention(embed_dim=D, num_heads=H, batch_first=True)\n",
    "\n",
    "# Random embeddings for the tokens\n",
    "x = torch.randn(B, L, D)\n",
    "\n",
    "# Apply attention with both masks\n",
    "out, attn_weights = attn(\n",
    "    x, x, x,\n",
    "    attn_mask=causal_mask,         # (L, L)\n",
    "    key_padding_mask=padding_mask  # (B, L)\n",
    ")\n",
    "\n",
    "print(\"Output shape:\", out.shape)                 # (B, L, D)\n",
    "print(\"Attention weights shape:\", attn_weights.shape) # (B*H, L, L)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Visualize causal mask\n",
    "# -------------------------------\n",
    "plt.imshow(causal_mask.numpy(), cmap=\"gray\")\n",
    "plt.title(\"Causal Attention Mask (white = -inf)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
