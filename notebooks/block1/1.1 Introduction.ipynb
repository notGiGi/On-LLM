{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff9d60c",
   "metadata": {},
   "source": [
    "# Block 1 — Mathematical Foundations of Tokenization, Batching & Attention Masking\n",
    "\n",
    "This notebook builds the **mathematical backbone** for how LLMs go from raw text to tokens, how batches are formed and losses computed, and how attention masks (causal/padding/head) enforce the same principles architecturally.\n",
    "\n",
    "---\n",
    "\n",
    "## What you will learn in Block 1\n",
    "1. **Objects & notation**: strings, normalization, tokenizer, vocabulary, special tokens.  \n",
    "2. **Autoregressive factorization** of sequence probability.  \n",
    "3. **Maximum-likelihood objective (NLL)** and its link to **cross-entropy**.  \n",
    "4. **Batching with padding** and the **masked mean token loss**.  \n",
    "5. **Perplexity** as exponential of mean NLL.  \n",
    "6. **Attention masking (preview)**: causal mask, key-padding mask, and head masking.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Objects and notation\n",
    "\n",
    "- **Alphabet & strings.**  \n",
    "  Let $\\Sigma$ be the raw character alphabet (e.g., Unicode code points).  \n",
    "  Any finite string is an element:\n",
    "  $$\n",
    "  x \\in \\Sigma^{*}.\n",
    "  $$\n",
    "\n",
    "- **Normalization.**  \n",
    "  A preprocessing map:\n",
    "  $$\n",
    "  \\nu:\\Sigma^{*} \\to \\Sigma^{*}, \\qquad x \\mapsto \\nu(x),\n",
    "  $$\n",
    "  that may include:\n",
    "  - Unicode normalization (e.g., NFKC),  \n",
    "  - case folding (lowercasing),  \n",
    "  - whitespace and punctuation rules.  \n",
    "\n",
    "  *Remark:* Normalization **changes the support** of the data distribution by altering how strings map to tokens (e.g., “Café” → “Cafe”).\n",
    "\n",
    "- **Tokenizer.**  \n",
    "  A **tokenizer** is a function that segments normalized text into discrete units (tokens) from a fixed vocabulary $\\mathcal{V}$.  \n",
    "  Mathematically:\n",
    "  $$\n",
    "  \\tau: \\Sigma^{*} \\to \\mathcal{V}^{*}, \\qquad\n",
    "  \\mathbf{t}=\\tau(\\nu(x))=(t_1,\\dots,t_{L(x)}), \\quad t_i \\in \\mathcal{V}.\n",
    "  $$\n",
    "\n",
    "  \n",
    "  - Tokenizers define the *basic symbols* LLMs see.  \n",
    "  - They can split text into **characters** (character-level), **words** (word-level), or **subwords** (BPE/Unigram).  \n",
    "  - Subword tokenization (e.g., “tokenization” → `[\"token\", \"ization\"]`) balances vocabulary size with sequence length.  \n",
    "  - This mapping is deterministic: the same string always yields the same token sequence.  \n",
    "  - Without tokenization, raw text would be unmanageable: the space of possible strings $\\Sigma^{*}$ is infinite, but the space of tokens $\\mathcal{V}$ is finite and tractable.\n",
    "\n",
    "- **Vocabulary & special tokens.**  \n",
    "  The vocabulary $\\mathcal{V}=\\{0,1,\\dots,V-1\\}$ contains learned tokens plus reserved IDs:\n",
    "  - `<bos>` : beginning of sequence  \n",
    "  - `<eos>` : end of sequence  \n",
    "  - `<pad>` : padding (used in batching)  \n",
    "  - `<unk>` : unknown token (for OOV strings)\n",
    "\n",
    "- **Augmented sequence (optional).**  \n",
    "  To mark boundaries explicitly:\n",
    "  $$\n",
    "  \\tilde{\\mathbf{t}} = (\\texttt{<bos>},\\, t_1,\\dots,t_{L(x)},\\, \\texttt{<eos>})\n",
    "  $$\n",
    "\n",
    "- **Dataset.**  \n",
    "  A dataset is a collection of $N$ independent samples:\n",
    "  $$\n",
    "  \\mathcal{D} = \\{\\, x^{(n)} \\,\\}_{n=1}^{N}.\n",
    "  $$\n",
    "  After tokenization:\n",
    "  $$\n",
    "  \\mathcal{D}_{\\text{tok}} = \\{\\, \\mathbf{t}^{(n)} \\,\\}_{n=1}^{N}, \\qquad\n",
    "  \\mathbf{t}^{(n)} = (t^{(n)}_1, \\dots, t^{(n)}_{L(x^{(n)})}).\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Autoregressive factorization (preview)\n",
    "\n",
    "An LLM with parameters $\\theta$ assigns probability to a tokenized string via the **causal chain rule**:\n",
    "\n",
    "$$\n",
    "p_\\theta(x) = \\prod_{i=1}^{L(x)} p_\\theta\\!\\big(t_i \\mid t_{<i}\\big),\n",
    "\\qquad t_{<i}=(t_1,\\dots,t_{i-1}).\n",
    "$$\n",
    "\n",
    "With explicit boundaries:\n",
    "$$\n",
    "p_\\theta(x) = \\prod_{i=1}^{\\tilde L} p_\\theta\\!\\big(\\tilde t_i \\mid \\tilde t_{<i}\\big),\n",
    "\\quad \\tilde t_1=\\texttt{<bos>},\\ \\tilde t_{\\tilde L}=\\texttt{<eos>}.\n",
    "$$\n",
    "\n",
    "*Interpretation:*  \n",
    "The model decomposes the text into **conditional next-token probabilities**; causality forbids peeking at the future.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training objective (preview)\n",
    "\n",
    "Given dataset $\\mathcal{D}$, **maximum likelihood** maximizes the log-probability assigned to the data:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta}\\; \\sum_{x \\in \\mathcal{D}} \\sum_{i=1}^{L(x)} \\log p_\\theta\\!\\big(t_i \\mid t_{<i}\\big).\n",
    "$$\n",
    "\n",
    "Equivalently, we **minimize the negative log-likelihood (NLL)**.  \n",
    "Later we will connect this to **softmax cross-entropy**, show the **masked mean over valid tokens** (excluding `<pad>`), and define **perplexity**:\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\!\\big(\\text{mean NLL per valid token}\\big).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Attention masking (preview)\n",
    "\n",
    "Self-attention computes scaled dot products between queries and keys.  \n",
    "To respect the mathematics above, we add:\n",
    "- a **causal mask** (no attending to future positions),  \n",
    "- a **key-padding mask** (ignore `<pad>` positions),  \n",
    "- and optionally a **head mask** (enable/disable heads for ablation/interpretability).\n",
    "\n",
    "Later we will show how these masks combine into the attention logits **before** the softmax.\n",
    "\n",
    "---\n",
    "\n",
    "### Outcome of Block 1\n",
    "\n",
    "By the end of Block 1 you will be able to:\n",
    "- Map raw strings $\\to$ tokens with clear assumptions about normalization and special tokens.  \n",
    "- Write and reason about $p_\\theta(x)$ via autoregressive factorization.  \n",
    "- Derive the MLE/NLL objective and relate it to cross-entropy.  \n",
    "- Implement padding-aware **masked mean** losses and compute **perplexity**.  \n",
    "- Explain how **attention masks** (causal/padding/head) enforce the same constraints architecturally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bedfd1",
   "metadata": {},
   "source": [
    "## 2) Autoregressive Factorization and Training Objective\n",
    "\n",
    "Now that we understand what **tokens** are and how a **tokenizer** converts text into discrete units, we ask:  \n",
    "**How does a Large Language Model assign a probability to a whole sequence of tokens?**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 The chain rule of probability\n",
    "\n",
    "From basic probability theory:  \n",
    "For any sequence of random variables $(X_1, X_2, \\dots, X_L)$ we can always write:\n",
    "\n",
    "$$\n",
    "p(X_1, X_2, \\dots, X_L) = \\prod_{i=1}^L p(X_i \\mid X_1, \\dots, X_{i-1}).\n",
    "$$\n",
    "\n",
    "This is called the **chain rule**.  \n",
    "It is an identity, not an assumption — it always holds.\n",
    "\n",
    "- $p(X_1)$ is the probability of the first element.  \n",
    "- $p(X_2 \\mid X_1)$ is the probability of the second given the first.  \n",
    "- $p(X_3 \\mid X_1, X_2)$ is the probability of the third given the first two.  \n",
    "- And so on.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Applying the chain rule to tokens\n",
    "\n",
    "For a token sequence $\\mathbf{t} = (t_1, t_2, \\dots, t_L)$ produced by the tokenizer:\n",
    "\n",
    "$$\n",
    "p_\\theta(\\mathbf{t}) = \\prod_{i=1}^L p_\\theta(t_i \\mid t_1, \\dots, t_{i-1}),\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the model parameters (the weights of the neural network).\n",
    "\n",
    "- $t_i$ is the $i$-th token.  \n",
    "- $t_{<i} = (t_1, \\dots, t_{i-1})$ is the prefix (all tokens before $i$).  \n",
    "- Each factor is the probability of the **next token** given all previous ones.\n",
    "\n",
    "With explicit boundary tokens `<bos>` (begin) and `<eos>` (end):\n",
    "\n",
    "$$\n",
    "p_\\theta(\\tilde{\\mathbf{t}}) = \\prod_{i=1}^{\\tilde L} p_\\theta(\\tilde t_i \\mid \\tilde t_{<i}),\n",
    "$$\n",
    "\n",
    "where $\\tilde L = L + 2$ because of the added boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Why autoregression?\n",
    "\n",
    "- **Causality.** Humans generate text left-to-right; the model imitates this by only looking at the past.  \n",
    "- **Simplicity.** Instead of modeling an entire sequence directly, we only need to model *next-token prediction*.  \n",
    "- **Flexibility.** With this formulation we can generate text token by token: sample $t_1$ from $p(t_1)$, then $t_2$ from $p(t_2\\mid t_1)$, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Training objective: Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "We want our model to assign **high probability** to real text sequences from the dataset.  \n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{ \\mathbf{t}^{(n)} \\}_{n=1}^N$, the **log-likelihood** is:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathcal{D}) = \\sum_{n=1}^N \\log p_\\theta(\\mathbf{t}^{(n)}).\n",
    "$$\n",
    "\n",
    "Using the autoregressive factorization:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathbf{t}^{(n)}) = \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}).\n",
    "$$\n",
    "\n",
    "Thus the training objective is:\n",
    "\n",
    "$$\n",
    "\\max_\\theta \\; \\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta\\!\\big(t^{(n)}_i \\mid t^{(n)}_{<i}\\big).\n",
    "$$\n",
    "\n",
    "This is called **Maximum Likelihood Estimation (MLE).**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 Why logs?\n",
    "\n",
    "If we worked directly with the product $\\prod p(t_i \\mid t_{<i})$, probabilities quickly become astronomically small (multiplying many numbers less than 1).  \n",
    "Taking the logarithm:\n",
    "\n",
    "- Turns products into sums (easier to compute).  \n",
    "- Stabilizes numerics (avoids underflow).  \n",
    "- Matches information-theoretic interpretations (log-loss).  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Negative log-likelihood (NLL)\n",
    "\n",
    "Instead of maximizing log-likelihood, we minimize the **negative log-likelihood**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\theta) = -\\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}).\n",
    "$$\n",
    "\n",
    "This is the **loss function** of LLMs.  \n",
    "Minimizing it means: *the model gets penalized whenever it assigns low probability to the correct next token.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.7 Connection to cross-entropy\n",
    "\n",
    "At each position $i$, the model outputs a probability distribution over the vocabulary $\\mathcal{V}$.  \n",
    "\n",
    "If the true token is $t_i$, the per-token loss is:\n",
    "\n",
    "$$\n",
    "\\ell_i = -\\log p_\\theta(t_i \\mid t_{<i}).\n",
    "$$\n",
    "\n",
    "This is exactly the **cross-entropy** between the predicted distribution $p_\\theta(\\cdot \\mid t_{<i})$ and the true one-hot distribution $y$ where $y_{k} = 1$ if $k=t_i$:\n",
    "\n",
    "$$\n",
    "\\ell_i = H(y, p_\\theta) = - \\sum_{k=1}^{V} y_k \\log p_\\theta(k \\mid t_{<i}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.8 Gradient identity (intuition for learning)\n",
    "\n",
    "The derivative of the cross-entropy with respect to the logits $z_{i,k}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_{i,k}} = p_\\theta(k \\mid t_{<i}) - y_k.\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- If the model assigns too much probability to the wrong token, the gradient pushes it down.  \n",
    "- If the model assigns too little to the correct token, the gradient pushes it up.  \n",
    "- Learning is simply **adjusting logits so that predicted probabilities match observed tokens.**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.9 Intuition\n",
    "\n",
    "- Training an LLM = **teaching it to be a good next-token predictor**.  \n",
    "- By predicting the next token well across billions of examples, the model learns grammar, semantics, style, reasoning patterns, and world knowledge.  \n",
    "- All of this comes from one simple principle: *minimize negative log-likelihood (maximize the probability of real text).*\n",
    "\n",
    "---\n",
    "\n",
    "**Next (Block 1.3):**  \n",
    "We extend this objective to **batches of sequences** (with different lengths), introduce **padding**, and show how to compute a **masked mean token loss** that ignores `<pad>` tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23429d",
   "metadata": {},
   "source": [
    "## 3) Batching, Padding, and the Masked Token Loss\n",
    "\n",
    "So far we looked at single sequences. But in practice, models are trained with **batches** of sequences in parallel, for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Why batching?\n",
    "\n",
    "- GPUs/TPUs can process many examples at once.  \n",
    "- Instead of updating weights after every single sequence, we group **B sequences** into a **batch**.  \n",
    "- This allows **vectorized operations** and faster convergence (by averaging gradients).\n",
    "\n",
    "Formally:  \n",
    "A batch $\\mathcal{B}$ is a set of $B$ token sequences:\n",
    "$$\n",
    "\\mathcal{B} = \\{\\mathbf{t}^{(1)}, \\mathbf{t}^{(2)}, \\dots, \\mathbf{t}^{(B)}\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 The problem of variable lengths\n",
    "\n",
    "Different sequences have different lengths:\n",
    "- Example: `\"The cat\"` → 2 tokens.  \n",
    "- Example: `\"A very long sentence here\"` → 6 tokens.  \n",
    "\n",
    "Neural networks expect tensors with the **same shape**, so we cannot stack them directly.  \n",
    "Solution: **Padding**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Padding sequences\n",
    "\n",
    "Let $L_{\\max}$ be the length of the longest sequence in the batch.  \n",
    "We build a matrix:\n",
    "\n",
    "$$\n",
    "T \\in \\mathbb{N}^{B \\times L_{\\max}}, \\quad\n",
    "T_{b,i} =\n",
    "\\begin{cases}\n",
    "t^{(b)}_i & \\text{if } i \\leq L(\\mathbf{t}^{(b)}), \\\\\\\\\n",
    "\\texttt{<pad>} & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Each row = one sequence.  \n",
    "- Shorter sequences are padded with a special `<pad>` token until they reach length $L_{\\max}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Mask of valid tokens\n",
    "\n",
    "To ensure padding does not affect the loss, we build a **mask**:\n",
    "\n",
    "$$\n",
    "M \\in \\{0,1\\}^{B \\times L_{\\max}}, \\qquad\n",
    "M_{b,i} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } T_{b,i} \\neq \\texttt{<pad>} \\quad (\\text{valid token}), \\\\\\\\\n",
    "0 & \\text{if } T_{b,i} = \\texttt{<pad>} \\quad (\\text{ignored}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Masked mean token loss\n",
    "\n",
    "Without masking, the loss would include `<pad>` tokens, corrupting training.  \n",
    "Instead we average only over **valid tokens**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{token}} =\n",
    "- \\frac{1}{N_{\\text{valid}}}\n",
    "\\sum_{b=1}^B \\sum_{i=1}^{L_{\\max}}\n",
    "M_{b,i} \\; \\log p_\\theta\\!\\big(T_{b,i} \\mid T_{b,<i}\\big),\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "N_{\\text{valid}} = \\sum_{b=1}^B \\sum_{i=1}^{L_{\\max}} M_{b,i}.\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "- Loss is **independent of how much padding there is**.  \n",
    "- Gradients reflect only *real tokens*.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.6 Implementation intuition\n",
    "\n",
    "Most deep learning frameworks implement this using:\n",
    "- A **CrossEntropyLoss** with `ignore_index=pad_id` (in PyTorch).  \n",
    "- An **attention_mask** to indicate valid tokens (in Transformers).\n",
    "\n",
    "Thus:\n",
    "- Forward pass uses the padded tensor.  \n",
    "- Loss computation ignores `<pad>`.  \n",
    "- Attention layers also ignore `<pad>` (via masking).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600f8717",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Suppose we have 3 sequences of different lengths\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose we have 3 sequences of different lengths\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],      # length 4\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id], # length 3\n",
    "    [12, 13, 14, 15, 16, 17]            # length 6 (max)\n",
    "])\n",
    "\n",
    "print(\"Batch shape:\", batch.shape)  # (3, 6)\n",
    "\n",
    "# Mask: 1 for valid tokens, 0 for pad\n",
    "mask = (batch != pad_id).long()\n",
    "print(\"Mask:\\n\", mask)\n",
    "\n",
    "# Example: logits from a model (random for demo)\n",
    "B, L, V = batch.shape[0], batch.shape[1], 20  # vocab size = 20\n",
    "logits = torch.randn(B, L, V)  # model output\n",
    "\n",
    "# Loss function that ignores pad tokens\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "# Shift targets by one position for next-token prediction\n",
    "targets = batch.clone()\n",
    "targets[targets == pad_id] = pad_id  # keep pad as pad\n",
    "print(\"Targets:\\n\", targets)\n",
    "\n",
    "# Flatten for CE loss\n",
    "loss = loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "print(\"Masked loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7eba7d",
   "metadata": {},
   "source": [
    "## 4) Perplexity: measuring how well a model predicts\n",
    "\n",
    "Up to now we focused on the **loss** (negative log-likelihood). But in language modeling, people often report **perplexity (PPL)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Definition\n",
    "\n",
    "For a dataset of $N_{\\text{valid}}$ valid tokens, perplexity is:\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\!\\Bigg( \\frac{1}{N_{\\text{valid}}}\n",
    "\\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})}\n",
    "-\\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}) \\Bigg).\n",
    "$$\n",
    "\n",
    "This is simply the exponential of the **average negative log-likelihood per token**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Intuition\n",
    "\n",
    "- If $\\mathrm{PPL}=1$: the model is perfect — it always assigns probability 1 to the correct token.  \n",
    "- If $\\mathrm{PPL}=V$ (vocabulary size): the model is as bad as random guessing.  \n",
    "- Lower perplexity = better model.\n",
    "\n",
    "In words: **perplexity measures “how many tokens the model is confused among, on average.”**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Why use perplexity?\n",
    "\n",
    "- It is easier to interpret than raw log-loss.  \n",
    "- It is standard in language modeling benchmarks (e.g., Penn Treebank, WikiText).  \n",
    "- It allows fair comparison across models trained with different vocabularies (to some extent).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Example with code\n",
    "Now let’s compute perplexity from the loss in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Suppose we have predictions for a batch\n",
    "pad_id = 0\n",
    "targets = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id],\n",
    "    [12, 13, 14, 15, 16, 17]\n",
    "])\n",
    "\n",
    "B, L = targets.shape\n",
    "V = 20\n",
    "logits = torch.randn(B, L, V)\n",
    "\n",
    "# Loss function that ignores padding\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id, reduction=\"sum\")\n",
    "\n",
    "# Compute total loss (sum over tokens, not mean yet)\n",
    "loss_sum = loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "\n",
    "# Count valid tokens (non-pad)\n",
    "valid_tokens = (targets != pad_id).sum().item()\n",
    "\n",
    "# Average NLL per token\n",
    "nll_avg = loss_sum.item() / valid_tokens\n",
    "\n",
    "# Perplexity\n",
    "ppl = math.exp(nll_avg)\n",
    "\n",
    "print(\"Average NLL per token:\", nll_avg)\n",
    "print(\"Perplexity:\", ppl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6e9b6",
   "metadata": {},
   "source": [
    "## 5) Attention Masking: Causal, Padding, and Head Masks\n",
    "\n",
    "So far we focused on **token probabilities** and **loss functions**.  \n",
    "Now we connect this theory to the **attention mechanism** inside Transformers — the core building block of LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 Self-attention in a nutshell\n",
    "\n",
    "In self-attention, every token can attend to every other token.  \n",
    "For a sequence of length $L$ and embedding size $d$, we compute:\n",
    "\n",
    "- **Queries:** $Q \\in \\mathbb{R}^{L \\times d}$  \n",
    "- **Keys:** $K \\in \\mathbb{R}^{L \\times d}$  \n",
    "- **Values:** $V \\in \\mathbb{R}^{L \\times d}$  \n",
    "\n",
    "The **attention scores** are:\n",
    "\n",
    "$$\n",
    "S = \\frac{QK^\\top}{\\sqrt{d}} \\quad \\in \\mathbb{R}^{L \\times L}.\n",
    "$$\n",
    "\n",
    "After applying softmax row-wise, we get the **attention weights**:\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}(S),\n",
    "$$\n",
    "\n",
    "and then the output is:\n",
    "\n",
    "$$\n",
    "O = A \\, V.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 The problem\n",
    "\n",
    "- Without restrictions, a token at position $i$ could attend to **future tokens** $j>i$.  \n",
    "- `<pad>` tokens could receive attention, contaminating representations.  \n",
    "- Sometimes we want to **disable certain heads** for ablation or interpretability.\n",
    "\n",
    "**Solution:** use **masks** that tell the model where it is allowed to attend.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Causal mask (autoregression)\n",
    "\n",
    "**Goal:** prevent a token from looking into the future.\n",
    "\n",
    "Definition:\n",
    "\n",
    "$$\n",
    "C \\in \\{0,1\\}^{L \\times L}, \\qquad\n",
    "C_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } j>i \\quad (\\text{future blocked}), \\\\\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Applied to scores:\n",
    "\n",
    "$$\n",
    "S \\;\\leftarrow\\; S + (-10^9)\\cdot C.\n",
    "$$\n",
    "\n",
    "This sets future positions to a very large negative value (≈$-\\infty$), so after softmax they get probability 0.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Padding mask\n",
    "\n",
    "**Goal:** ignore `<pad>` tokens added during batching.\n",
    "\n",
    "Definition for a batch of size $B$:\n",
    "\n",
    "$$\n",
    "P \\in \\{0,1\\}^{B \\times L}, \\qquad\n",
    "P_{b,j} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if token $j$ in sequence $b$ is <pad>}, \\\\\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "When broadcasted into attention scores, positions marked with 1 are also masked out.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Head mask\n",
    "\n",
    "**Goal:** enable or disable specific heads.\n",
    "\n",
    "For $H$ attention heads:\n",
    "\n",
    "$$\n",
    "h \\in \\{0,1\\}^H \\quad \\text{(or } h \\in \\{0,1\\}^{B \\times H}\\text{ if per-example)}.\n",
    "$$\n",
    "\n",
    "- $h_h=0$ → that head is disabled (all its contributions zeroed).  \n",
    "- Useful for **interpretability** (probing which heads matter), or for **structured pruning**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.6 Combined masking\n",
    "\n",
    "For a full batch, the final scores are:\n",
    "\n",
    "$$\n",
    "S = \\frac{QK^\\top}{\\sqrt{d}}.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Broadcast $C$ (causal mask) to $(1,1,L,L)$.  \n",
    "2. Broadcast $P$ (padding mask) to $(B,1,1,L)$.  \n",
    "3. Combine:  \n",
    "   $$\n",
    "   M = \\text{broadcast}(C) \\;\\lor\\; \\text{broadcast}(P).\n",
    "   $$\n",
    "4. Apply to scores:  \n",
    "   $$\n",
    "   S \\;\\leftarrow\\; S + (-10^9)\\cdot M.\n",
    "   $$\n",
    "5. Softmax row-wise over keys:  \n",
    "   $$\n",
    "   A = \\text{softmax}(S).\n",
    "   $$\n",
    "6. Apply head mask (if any):  \n",
    "   $$\n",
    "   A \\;\\leftarrow\\; A \\odot \\text{broadcast}(h).\n",
    "   $$\n",
    "7. Final output:  \n",
    "   $$\n",
    "   O = A \\, V.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.7 Why attention masking is essential\n",
    "\n",
    "- **Causal mask:** enforces the autoregressive factorization (no cheating by looking at the future).  \n",
    "- **Padding mask:** ensures that `<pad>` tokens do not leak into the computation.  \n",
    "- **Head mask:** gives flexibility for analysis and pruning.\n",
    "\n",
    "Together, these masks align the **mathematics of training (loss + padding)** with the **architecture of attention**.\n",
    "\n",
    "---\n",
    "\n",
    "**Next (Block 1.6):**  \n",
    "We will implement these masks in **PyTorch code**:  \n",
    "1. Build a causal mask matrix.  \n",
    "2. Build a padding mask from a batch with `<pad>`.  \n",
    "3. Show how they are applied inside a MultiheadAttention layer.  \n",
    "4. Visualize the causal mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ff7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Create toy batch (with padding)\n",
    "# -------------------------------\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],      # length 4\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id], # length 3\n",
    "    [12, 13, 14, 15, 16, 17]            # length 6 (max)\n",
    "])\n",
    "\n",
    "B, L = batch.shape\n",
    "D = 16   # embedding dimension\n",
    "H = 2    # number of heads\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build causal mask\n",
    "# -------------------------------\n",
    "# Shape: (L, L)\n",
    "causal_mask = torch.triu(torch.ones(L, L) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "print(\"Causal mask:\\n\", causal_mask)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Build key-padding mask\n",
    "# -------------------------------\n",
    "# True where PAD tokens are present\n",
    "padding_mask = (batch == pad_id)\n",
    "print(\"Padding mask shape:\", padding_mask.shape)  # (B, L)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. MultiheadAttention with masks\n",
    "# -------------------------------\n",
    "attn = nn.MultiheadAttention(embed_dim=D, num_heads=H, batch_first=True)\n",
    "\n",
    "# Random embeddings for the tokens\n",
    "x = torch.randn(B, L, D)\n",
    "\n",
    "# Apply attention with both masks\n",
    "out, attn_weights = attn(\n",
    "    x, x, x,\n",
    "    attn_mask=causal_mask,         # (L, L)\n",
    "    key_padding_mask=padding_mask  # (B, L)\n",
    ")\n",
    "\n",
    "print(\"Output shape:\", out.shape)                 # (B, L, D)\n",
    "print(\"Attention weights shape:\", attn_weights.shape) # (B*H, L, L)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Visualize causal mask\n",
    "# -------------------------------\n",
    "plt.imshow(causal_mask.numpy(), cmap=\"gray\")\n",
    "plt.title(\"Causal Attention Mask (white = -inf)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf020b",
   "metadata": {},
   "source": [
    "### 6.5 Complete attention implementation with fundamental masking\n",
    "\n",
    "**Essential mathematical components:**  \n",
    "We now implement a **minimal but complete** attention mechanism that demonstrates the core masking principles from our mathematical foundations.\n",
    "\n",
    "**Key requirements:**\n",
    "1. **Causal masking**: Enforce $p(t_i | t_{<i})$ by preventing future access\n",
    "2. **Padding masking**: Ignore `<pad>` tokens during attention computation  \n",
    "3. **Correct softmax normalization**: Ensure $\\sum_{j \\in \\text{valid}} A_{ij} = 1$\n",
    "\n",
    "**Implementation principle:**  \n",
    "Apply masks to attention scores **before softmax**, then verify that the mathematical properties we derived are satisfied in practice.\n",
    "\n",
    "$$\n",
    "\\text{scores}_{\\text{masked}} = \\text{scores}_{\\text{raw}} + \\text{causal\\_mask} + \\text{padding\\_mask}\n",
    "$$\n",
    "$$\n",
    "A = \\text{softmax}(\\text{scores}_{\\text{masked}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_masked_attention(embeddings, causal_mask, key_padding_mask):\n",
    "    \"\"\"\n",
    "    Minimal implementation demonstrating core masking principles.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: (B, L, d) input embeddings\n",
    "        causal_mask: (L, L) causal constraint matrix\n",
    "        key_padding_mask: (B, L) boolean mask for padding\n",
    "    \n",
    "    Returns:\n",
    "        output: (B, L, d) attention output\n",
    "        attention_weights: (B, L, L) for visualization\n",
    "    \"\"\"\n",
    "    B, L, d = embeddings.shape\n",
    "    \n",
    "    # Simple attention: use embeddings directly as Q, K, V for demonstration\n",
    "    Q = K = V = embeddings\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d ** 0.5)  # (B, L, L)\n",
    "    \n",
    "    # Apply causal mask: broadcast (L, L) -> (B, L, L)\n",
    "    if causal_mask is not None:\n",
    "        scores = scores + causal_mask.unsqueeze(0)\n",
    "    \n",
    "    # Apply padding mask: broadcast (B, L) -> (B, L, L)  \n",
    "    if key_padding_mask is not None:\n",
    "        # Expand to (B, L, L) - we mask the key dimension (last dim)\n",
    "        expanded_mask = key_padding_mask.unsqueeze(1).expand(-1, L, -1)\n",
    "        scores = scores.masked_fill(expanded_mask, float('-inf'))\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply to values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test with our batch\n",
    "d_embed = 8  # Small embedding dimension for clarity\n",
    "embeddings = torch.randn(B, L, d_embed)\n",
    "\n",
    "# Apply our masking\n",
    "output, attn_weights = simple_masked_attention(\n",
    "    embeddings, causal_mask, key_padding_mask\n",
    ")\n",
    "\n",
    "print(f\"Simple attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Quick verification: check that attention sums to 1 over valid positions\n",
    "print(\"\\nVerification - attention sums for first sequence:\")\n",
    "for i in range(min(L, 4)):\n",
    "    # Valid positions: not padding AND causal constraint\n",
    "    valid_keys = ~key_padding_mask[0] & (torch.arange(L) <= i)\n",
    "    if valid_keys.any():\n",
    "        attn_sum = attn_weights[0, i, valid_keys].sum().item()\n",
    "        print(f\"  Query {i}: sum over valid keys = {attn_sum:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3c097",
   "metadata": {},
   "source": [
    "### 6.6 Visualizing attention masks and patterns\n",
    "\n",
    "**Visualization objectives:**\n",
    "1. **Understand mask structure**: See how causal and padding constraints look\n",
    "2. **Verify mask application**: Confirm that attention respects our constraints  \n",
    "3. **Interpret attention patterns**: Connect math to intuitive understanding\n",
    "\n",
    "**What to look for:**\n",
    "- **Lower triangular pattern**: Causal masking creates this structure\n",
    "- **Zero attention to padding**: Padded positions should receive no attention\n",
    "- **Probability conservation**: Each row sums to 1 over valid positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeca90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_attention_fundamentals(attention_weights, batch_tokens, causal_mask, key_padding_mask, pad_id):\n",
    "    \"\"\"\n",
    "    Create fundamental visualizations for understanding attention masking.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot 1: Causal mask structure\n",
    "    axes[0,0].imshow(causal_mask.numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[0,0].set_title('Causal Mask\\n(white = blocked, blue = allowed)')\n",
    "    axes[0,0].set_xlabel('Key positions')\n",
    "    axes[0,0].set_ylabel('Query positions')\n",
    "    \n",
    "    # Plot 2: Key padding mask for first sequence\n",
    "    pad_viz = key_padding_mask[0].float().unsqueeze(0).numpy()\n",
    "    axes[0,1].imshow(pad_viz, cmap='RdBu_r', aspect='auto')\n",
    "    axes[0,1].set_title(f'Key Padding Mask (Seq 0)\\n(red = padding)')\n",
    "    axes[0,1].set_xlabel('Key positions')\n",
    "    axes[0,1].set_yticks([])\n",
    "    \n",
    "    # Add token labels\n",
    "    tokens = batch_tokens[0].numpy()\n",
    "    labels = []\n",
    "    for token_id in tokens:\n",
    "        if token_id == pad_id:\n",
    "            labels.append('PAD')\n",
    "        elif token_id == BOS_ID:\n",
    "            labels.append('BOS')\n",
    "        elif token_id == EOS_ID:\n",
    "            labels.append('EOS')\n",
    "        else:\n",
    "            labels.append(f't{token_id}')\n",
    "    \n",
    "    axes[0,1].set_xticks(range(len(labels)))\n",
    "    axes[0,1].set_xticklabels(labels, rotation=45)\n",
    "    \n",
    "    # Plot 3: Actual attention weights (first sequence)\n",
    "    attn_viz = attention_weights[0].numpy()\n",
    "    im3 = axes[1,0].imshow(attn_viz, cmap='Blues', vmin=0, vmax=attn_viz.max())\n",
    "    axes[1,0].set_title('Attention Weights (Seq 0)\\nafter masking')\n",
    "    axes[1,0].set_xlabel('Key positions')\n",
    "    axes[1,0].set_ylabel('Query positions')\n",
    "    axes[1,0].set_xticks(range(len(labels)))\n",
    "    axes[1,0].set_xticklabels(labels, rotation=45)\n",
    "    axes[1,0].set_yticks(range(len(labels)))\n",
    "    axes[1,0].set_yticklabels(labels)\n",
    "    plt.colorbar(im3, ax=axes[1,0], fraction=0.046)\n",
    "    \n",
    "    # Plot 4: Attention distribution for a specific query\n",
    "    query_idx = 3  # Look at query position 3\n",
    "    valid_keys = ~key_padding_mask[0] & (torch.arange(L) <= query_idx)\n",
    "    \n",
    "    axes[1,1].bar(range(L), attention_weights[0, query_idx].numpy(), alpha=0.7)\n",
    "    axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Highlight valid vs invalid positions\n",
    "    for i, (is_valid, label) in enumerate(zip(valid_keys, labels)):\n",
    "        color = 'green' if is_valid else 'red'\n",
    "        axes[1,1].axvline(x=i, color=color, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    axes[1,1].set_title(f'Attention Distribution\\nQuery position {query_idx}')\n",
    "    axes[1,1].set_xlabel('Key positions')\n",
    "    axes[1,1].set_ylabel('Attention weight')\n",
    "    axes[1,1].set_xticks(range(len(labels)))\n",
    "    axes[1,1].set_xticklabels(labels, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical verification\n",
    "    print(\"=== Numerical Verification ===\")\n",
    "    print(f\"Query {query_idx} attention sum: {attention_weights[0, query_idx, valid_keys].sum():.6f}\")\n",
    "    print(f\"Attention to future positions: {attention_weights[0, query_idx, query_idx+1:].sum():.8f}\")\n",
    "    print(f\"Attention to padding: {attention_weights[0, query_idx, key_padding_mask[0]].sum():.8f}\")\n",
    "\n",
    "# Create the visualization\n",
    "visualize_attention_fundamentals(attn_weights, batch_tokens, causal_mask, key_padding_mask, PAD_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dd174",
   "metadata": {},
   "source": [
    "### 6.7 Practical exercises with the tiny corpus\n",
    "\n",
    "Now we apply our masking concepts to the **tiny corpus** from the data folder. This bridges theory with practice using real tokenized text.\n",
    "\n",
    "**Learning objectives:**\n",
    "1. **Tokenize real text** and apply masking\n",
    "2. **Compute masked losses** on actual sequences  \n",
    "3. **Observe attention patterns** on linguistic content\n",
    "4. **Verify mathematical properties** hold for real data\n",
    "\n",
    "**Tiny corpus content:**\n",
    "- `\"the quick brown fox jumps over the lazy dog\"`\n",
    "- `\"abracadabra abracadabra token tokenization\"`  \n",
    "- `\"subword models like bpe and unigram are common\"`\n",
    "\n",
    "This gives us realistic sequence lengths and repeated tokens to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the tiny corpus\n",
    "def load_tiny_corpus():\n",
    "    \"\"\"Load the tiny corpus and create a simple tokenizer.\"\"\"\n",
    "    \n",
    "    # Read the tiny corpus (from the data/ folder)\n",
    "    corpus_text = \"\"\"the quick brown fox jumps over the lazy dog\n",
    "abracadabra abracadabra token tokenization\n",
    "subword models like bpe and unigram are common\"\"\"\n",
    "    \n",
    "    # Simple word-level tokenization\n",
    "    sentences = corpus_text.strip().split('\\n')\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab = {'<pad>': 0, '<bos>': 1, '<eos>': 2}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    tokenized = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [vocab['<bos>']]\n",
    "        tokens.extend(vocab[word] for word in sentence.split())\n",
    "        tokens.append(vocab['<eos>'])\n",
    "        tokenized.append(tokens)\n",
    "    \n",
    "    return tokenized, vocab\n",
    "\n",
    "# Process the corpus\n",
    "tokenized_corpus, vocab = load_tiny_corpus()\n",
    "vocab_size = len(vocab)\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(\"=== Tiny Corpus Analysis ===\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {list(vocab.keys())}\")\n",
    "print(\"\\nTokenized sentences:\")\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    words = [id_to_token[token_id] for token_id in tokens]\n",
    "    print(f\"  {i}: {tokens} -> {words}\")\n",
    "\n",
    "# Create a padded batch from the corpus\n",
    "max_len = max(len(seq) for seq in tokenized_corpus)\n",
    "corpus_batch = torch.zeros(len(tokenized_corpus), max_len, dtype=torch.long)\n",
    "\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    corpus_batch[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    # Remaining positions are already 0 (PAD_ID)\n",
    "\n",
    "print(f\"\\nCorpus batch shape: {corpus_batch.shape}\")\n",
    "print(\"Corpus batch:\")\n",
    "print(corpus_batch)\n",
    "\n",
    "# Create masks for the corpus\n",
    "corpus_causal_mask = create_causal_mask(max_len)\n",
    "corpus_padding_mask = create_key_padding_mask(corpus_batch, PAD_ID)\n",
    "\n",
    "print(f\"\\nMask shapes:\")\n",
    "print(f\"  Causal: {corpus_causal_mask.shape}\")\n",
    "print(f\"  Padding: {corpus_padding_mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35b7f3",
   "metadata": {},
   "source": [
    "### 6.8 Block 1 Summary and Transition\n",
    "\n",
    "**What we accomplished in Block 1:**\n",
    "\n",
    "**Mathematical Foundations** ✅\n",
    "- Formalized tokenization: $\\tau: \\Sigma^* \\to \\mathcal{V}^*$\n",
    "- Derived autoregressive factorization: $p_\\theta(x) = \\prod_{i=1}^L p_\\theta(t_i | t_{<i})$\n",
    "- Connected MLE to cross-entropy loss: $\\mathcal{L} = -\\sum \\log p_\\theta(t_i | t_{<i})$\n",
    "\n",
    "**Batching Mathematics** ✅  \n",
    "- Formalized padding and masked token loss\n",
    "- Proved padding invariance with proper masking\n",
    "- Computed perplexity as $\\exp(\\text{mean NLL})$\n",
    "\n",
    "**Attention Masking Theory** ✅\n",
    "- Proved necessity of causal masking for autoregressive consistency\n",
    "- Derived key-padding mask for variable-length sequences\n",
    "- Implemented and verified mask composition\n",
    "\n",
    "**Key Mathematical Insights:**\n",
    "1. **Masking is not optional** - it's required for mathematical consistency\n",
    "2. **Padding creates measure-zero artifacts** that must be excluded from loss\n",
    "3. **Attention weights form probability simplexes** over valid positions only\n",
    "4. **Causal constraints preserve information-theoretic bottleneck** properties\n",
    "\n",
    "---\n",
    "\n",
    "**Transition to Block 2: Transformer Architecture**\n",
    "\n",
    "Block 2 will build on these foundations to explore:\n",
    "- **Complete transformer layers** (LayerNorm, residuals, FFN)\n",
    "- **Positional encodings** and their mathematical properties  \n",
    "- **Multi-layer composition** and representation learning theory\n",
    "- **Implementation from scratch** with comprehensive analysis\n",
    "\n",
    "The mathematical rigor established here provides the foundation for understanding **why** transformers work, not just **how** they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd99d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: compute masked loss on tiny corpus\n",
    "def compute_corpus_loss(batch_tokens, vocab_size, causal_mask, padding_mask):\n",
    "    \"\"\"\n",
    "    Compute the masked language modeling loss on our tiny corpus.\n",
    "    This demonstrates the complete pipeline from Block 1.\n",
    "    \"\"\"\n",
    "    B, L = batch_tokens.shape\n",
    "    \n",
    "    # Create dummy \"model predictions\" (random logits)\n",
    "    # In reality, these would come from a transformer\n",
    "    logits = torch.randn(B, L, vocab_size) * 0.1\n",
    "    \n",
    "    # Targets for next-token prediction: shift input by 1\n",
    "    targets = batch_tokens.clone()\n",
    "    \n",
    "    # Compute cross-entropy loss with padding ignored\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, reduction='none')\n",
    "    \n",
    "    # Reshape for loss computation\n",
    "    logits_flat = logits.view(-1, vocab_size)  # (B*L, V)\n",
    "    targets_flat = targets.view(-1)             # (B*L,)\n",
    "    \n",
    "    # Compute per-token losses\n",
    "    token_losses = loss_fn(logits_flat, targets_flat)  # (B*L,)\n",
    "    token_losses = token_losses.view(B, L)             # (B, L)\n",
    "    \n",
    "    # Apply padding mask to exclude PAD tokens from loss\n",
    "    valid_tokens = ~padding_mask\n",
    "    masked_losses = token_losses * valid_tokens.float()\n",
    "    \n",
    "    # Compute mean loss over valid tokens only\n",
    "    total_loss = masked_losses.sum()\n",
    "    total_valid = valid_tokens.sum()\n",
    "    mean_loss = total_loss / total_valid\n",
    "    \n",
    "    # Compute perplexity\n",
    "    perplexity = torch.exp(mean_loss)\n",
    "    \n",
    "    return mean_loss, perplexity, token_losses, valid_tokens\n",
    "\n",
    "# Apply to our corpus\n",
    "corpus_loss, corpus_ppl, token_losses, valid_mask = compute_corpus_loss(\n",
    "    corpus_batch, vocab_size, corpus_causal_mask, corpus_padding_mask\n",
    ")\n",
    "\n",
    "print(\"=== Final Block 1 Demonstration ===\")\n",
    "print(f\"Corpus mean loss: {corpus_loss:.4f}\")\n",
    "print(f\"Corpus perplexity: {corpus_ppl:.4f}\")\n",
    "print(f\"Total valid tokens: {valid_mask.sum().item()}\")\n",
    "\n",
    "print(\"\\nPer-sequence analysis:\")\n",
    "for i in range(len(tokenized_corpus)):\n",
    "    seq_valid = valid_mask[i]\n",
    "    seq_loss = token_losses[i, seq_valid].mean()\n",
    "    seq_ppl = torch.exp(seq_loss)\n",
    "    tokens = [id_to_token[tid] for tid in corpus_batch[i, seq_valid].tolist()]\n",
    "    \n",
    "    print(f\"  Sequence {i}: loss={seq_loss:.4f}, ppl={seq_ppl:.4f}\")\n",
    "    print(f\"             tokens: {tokens}\")\n",
    "\n",
    "print(f\"\\n🎓 Block 1 Complete!\")\n",
    "print(f\"   ✅ Mathematical foundations established\")  \n",
    "print(f\"   ✅ Masking theory and implementation verified\")\n",
    "print(f\"   ✅ Ready for Block 2: Transformer Architecture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
