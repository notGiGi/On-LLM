{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff9d60c",
   "metadata": {},
   "source": [
    "# Block 1 — Mathematical Foundations of Tokenization, Batching & Attention Masking\n",
    "\n",
    "This notebook builds the **mathematical backbone** for how LLMs go from raw text to tokens, how batches are formed and losses computed, and how attention masks (causal/padding/head) enforce the same principles architecturally.\n",
    "\n",
    "---\n",
    "\n",
    "## What you will learn in Block 1\n",
    "1. **Objects & notation**: strings, normalization, tokenizer, vocabulary, special tokens.  \n",
    "2. **Autoregressive factorization** of sequence probability.  \n",
    "3. **Maximum-likelihood objective (NLL)** and its link to **cross-entropy**.  \n",
    "4. **Batching with padding** and the **masked mean token loss**.  \n",
    "5. **Perplexity** as exponential of mean NLL.  \n",
    "6. **Attention masking (preview)**: causal mask, key-padding mask, and head masking.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Objects and notation\n",
    "\n",
    "- **Alphabet & strings.**  \n",
    "  Let $\\Sigma$ be the raw character alphabet (e.g., Unicode code points).  \n",
    "  Any finite string is an element:\n",
    "  $$\n",
    "  x \\in \\Sigma^{*}.\n",
    "  $$\n",
    "\n",
    "- **Normalization.**  \n",
    "  A preprocessing map:\n",
    "  $$\n",
    "  \\nu:\\Sigma^{*} \\to \\Sigma^{*}, \\qquad x \\mapsto \\nu(x),\n",
    "  $$\n",
    "  that may include:\n",
    "  - Unicode normalization (e.g., NFKC),  \n",
    "  - case folding (lowercasing),  \n",
    "  - whitespace and punctuation rules.  \n",
    "\n",
    "  *Remark:* Normalization **changes the support** of the data distribution by altering how strings map to tokens (e.g., “Café” → “Cafe”).\n",
    "\n",
    "- **Tokenizer.**  \n",
    "  A **tokenizer** is a function that segments normalized text into discrete units (tokens) from a fixed vocabulary $\\mathcal{V}$.  \n",
    "  Mathematically:\n",
    "  $$\n",
    "  \\tau: \\Sigma^{*} \\to \\mathcal{V}^{*}, \\qquad\n",
    "  \\mathbf{t}=\\tau(\\nu(x))=(t_1,\\dots,t_{L(x)}), \\quad t_i \\in \\mathcal{V}.\n",
    "  $$\n",
    "\n",
    "  \n",
    "  - Tokenizers define the *basic symbols* LLMs see.  \n",
    "  - They can split text into **characters** (character-level), **words** (word-level), or **subwords** (BPE/Unigram).  \n",
    "  - Subword tokenization (e.g., “tokenization” → `[\"token\", \"ization\"]`) balances vocabulary size with sequence length.  \n",
    "  - This mapping is deterministic: the same string always yields the same token sequence.  \n",
    "  - Without tokenization, raw text would be unmanageable: the space of possible strings $\\Sigma^{*}$ is infinite, but the space of tokens $\\mathcal{V}$ is finite and tractable.\n",
    "\n",
    "- **Vocabulary & special tokens.**  \n",
    "  The vocabulary $\\mathcal{V}=\\{0,1,\\dots,V-1\\}$ contains learned tokens plus reserved IDs:\n",
    "  - `<bos>` : beginning of sequence  \n",
    "  - `<eos>` : end of sequence  \n",
    "  - `<pad>` : padding (used in batching)  \n",
    "  - `<unk>` : unknown token (for OOV strings)\n",
    "\n",
    "- **Augmented sequence (optional).**  \n",
    "  To mark boundaries explicitly:\n",
    "  $$\n",
    "  \\tilde{\\mathbf{t}} = (\\texttt{<bos>},\\, t_1,\\dots,t_{L(x)},\\, \\texttt{<eos>})\n",
    "  $$\n",
    "\n",
    "- **Dataset.**  \n",
    "  A dataset is a collection of $N$ independent samples:\n",
    "  $$\n",
    "  \\mathcal{D} = \\{\\, x^{(n)} \\,\\}_{n=1}^{N}.\n",
    "  $$\n",
    "  After tokenization:\n",
    "  $$\n",
    "  \\mathcal{D}_{\\text{tok}} = \\{\\, \\mathbf{t}^{(n)} \\,\\}_{n=1}^{N}, \\qquad\n",
    "  \\mathbf{t}^{(n)} = (t^{(n)}_1, \\dots, t^{(n)}_{L(x^{(n)})}).\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Autoregressive factorization (preview)\n",
    "\n",
    "An LLM with parameters $\\theta$ assigns probability to a tokenized string via the **causal chain rule**:\n",
    "\n",
    "$$\n",
    "p_\\theta(x) = \\prod_{i=1}^{L(x)} p_\\theta\\!\\big(t_i \\mid t_{<i}\\big),\n",
    "\\qquad t_{<i}=(t_1,\\dots,t_{i-1}).\n",
    "$$\n",
    "\n",
    "With explicit boundaries:\n",
    "$$\n",
    "p_\\theta(x) = \\prod_{i=1}^{\\tilde L} p_\\theta\\!\\big(\\tilde t_i \\mid \\tilde t_{<i}\\big),\n",
    "\\quad \\tilde t_1=\\texttt{<bos>},\\ \\tilde t_{\\tilde L}=\\texttt{<eos>}.\n",
    "$$\n",
    "\n",
    "*Interpretation:*  \n",
    "The model decomposes the text into **conditional next-token probabilities**; causality forbids peeking at the future.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training objective (preview)\n",
    "\n",
    "Given dataset $\\mathcal{D}$, **maximum likelihood** maximizes the log-probability assigned to the data:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta}\\; \\sum_{x \\in \\mathcal{D}} \\sum_{i=1}^{L(x)} \\log p_\\theta\\!\\big(t_i \\mid t_{<i}\\big).\n",
    "$$\n",
    "\n",
    "Equivalently, we **minimize the negative log-likelihood (NLL)**.  \n",
    "Later we will connect this to **softmax cross-entropy**, show the **masked mean over valid tokens** (excluding `<pad>`), and define **perplexity**:\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\!\\big(\\text{mean NLL per valid token}\\big).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Attention masking (preview)\n",
    "\n",
    "Self-attention computes scaled dot products between queries and keys.  \n",
    "To respect the mathematics above, we add:\n",
    "- a **causal mask** (no attending to future positions),  \n",
    "- a **key-padding mask** (ignore `<pad>` positions),  \n",
    "- and optionally a **head mask** (enable/disable heads for ablation/interpretability).\n",
    "\n",
    "Later we will show how these masks combine into the attention logits **before** the softmax.\n",
    "\n",
    "---\n",
    "\n",
    "### Outcome of Block 1\n",
    "\n",
    "By the end of Block 1 you will be able to:\n",
    "- Map raw strings $\\to$ tokens with clear assumptions about normalization and special tokens.  \n",
    "- Write and reason about $p_\\theta(x)$ via autoregressive factorization.  \n",
    "- Derive the MLE/NLL objective and relate it to cross-entropy.  \n",
    "- Implement padding-aware **masked mean** losses and compute **perplexity**.  \n",
    "- Explain how **attention masks** (causal/padding/head) enforce the same constraints architecturally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bedfd1",
   "metadata": {},
   "source": [
    "## 2) Autoregressive Factorization and Training Objective\n",
    "\n",
    "Now that we understand what **tokens** are and how a **tokenizer** converts text into discrete units, we ask:  \n",
    "**How does a Large Language Model assign a probability to a whole sequence of tokens?**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 The chain rule of probability\n",
    "\n",
    "From basic probability theory:  \n",
    "For any sequence of random variables $(X_1, X_2, \\dots, X_L)$ we can always write:\n",
    "\n",
    "$$\n",
    "p(X_1, X_2, \\dots, X_L) = \\prod_{i=1}^L p(X_i \\mid X_1, \\dots, X_{i-1}).\n",
    "$$\n",
    "\n",
    "This is called the **chain rule**.  \n",
    "It is an identity, not an assumption — it always holds.\n",
    "\n",
    "- $p(X_1)$ is the probability of the first element.  \n",
    "- $p(X_2 \\mid X_1)$ is the probability of the second given the first.  \n",
    "- $p(X_3 \\mid X_1, X_2)$ is the probability of the third given the first two.  \n",
    "- And so on.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Applying the chain rule to tokens\n",
    "\n",
    "For a token sequence $\\mathbf{t} = (t_1, t_2, \\dots, t_L)$ produced by the tokenizer:\n",
    "\n",
    "$$\n",
    "p_\\theta(\\mathbf{t}) = \\prod_{i=1}^L p_\\theta(t_i \\mid t_1, \\dots, t_{i-1}),\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the model parameters (the weights of the neural network).\n",
    "\n",
    "- $t_i$ is the $i$-th token.  \n",
    "- $t_{<i} = (t_1, \\dots, t_{i-1})$ is the prefix (all tokens before $i$).  \n",
    "- Each factor is the probability of the **next token** given all previous ones.\n",
    "\n",
    "With explicit boundary tokens `<bos>` (begin) and `<eos>` (end):\n",
    "\n",
    "$$\n",
    "p_\\theta(\\tilde{\\mathbf{t}}) = \\prod_{i=1}^{\\tilde L} p_\\theta(\\tilde t_i \\mid \\tilde t_{<i}),\n",
    "$$\n",
    "\n",
    "where $\\tilde L = L + 2$ because of the added boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Why autoregression?\n",
    "\n",
    "- **Causality.** Humans generate text left-to-right; the model imitates this by only looking at the past.  \n",
    "- **Simplicity.** Instead of modeling an entire sequence directly, we only need to model *next-token prediction*.  \n",
    "- **Flexibility.** With this formulation we can generate text token by token: sample $t_1$ from $p(t_1)$, then $t_2$ from $p(t_2\\mid t_1)$, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Training objective: Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "We want our model to assign **high probability** to real text sequences from the dataset.  \n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{ \\mathbf{t}^{(n)} \\}_{n=1}^N$, the **log-likelihood** is:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathcal{D}) = \\sum_{n=1}^N \\log p_\\theta(\\mathbf{t}^{(n)}).\n",
    "$$\n",
    "\n",
    "Using the autoregressive factorization:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathbf{t}^{(n)}) = \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}).\n",
    "$$\n",
    "\n",
    "Thus the training objective is:\n",
    "\n",
    "$$\n",
    "\\max_\\theta \\; \\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta\\!\\big(t^{(n)}_i \\mid t^{(n)}_{<i}\\big).\n",
    "$$\n",
    "\n",
    "This is called **Maximum Likelihood Estimation (MLE).**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 Why logs?\n",
    "\n",
    "If we worked directly with the product $\\prod p(t_i \\mid t_{<i})$, probabilities quickly become astronomically small (multiplying many numbers less than 1).  \n",
    "Taking the logarithm:\n",
    "\n",
    "- Turns products into sums (easier to compute).  \n",
    "- Stabilizes numerics (avoids underflow).  \n",
    "- Matches information-theoretic interpretations (log-loss).  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Negative log-likelihood (NLL)\n",
    "\n",
    "Instead of maximizing log-likelihood, we minimize the **negative log-likelihood**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\theta) = -\\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}).\n",
    "$$\n",
    "\n",
    "This is the **loss function** of LLMs.  \n",
    "Minimizing it means: *the model gets penalized whenever it assigns low probability to the correct next token.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.7 Connection to cross-entropy\n",
    "\n",
    "At each position $i$, the model outputs a probability distribution over the vocabulary $\\mathcal{V}$.  \n",
    "\n",
    "If the true token is $t_i$, the per-token loss is:\n",
    "\n",
    "$$\n",
    "\\ell_i = -\\log p_\\theta(t_i \\mid t_{<i}).\n",
    "$$\n",
    "\n",
    "This is exactly the **cross-entropy** between the predicted distribution $p_\\theta(\\cdot \\mid t_{<i})$ and the true one-hot distribution $y$ where $y_{k} = 1$ if $k=t_i$:\n",
    "\n",
    "$$\n",
    "\\ell_i = H(y, p_\\theta) = - \\sum_{k=1}^{V} y_k \\log p_\\theta(k \\mid t_{<i}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.8 Gradient identity (intuition for learning)\n",
    "\n",
    "The derivative of the cross-entropy with respect to the logits $z_{i,k}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_{i,k}} = p_\\theta(k \\mid t_{<i}) - y_k.\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- If the model assigns too much probability to the wrong token, the gradient pushes it down.  \n",
    "- If the model assigns too little to the correct token, the gradient pushes it up.  \n",
    "- Learning is simply **adjusting logits so that predicted probabilities match observed tokens.**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.9 Intuition\n",
    "\n",
    "- Training an LLM = **teaching it to be a good next-token predictor**.  \n",
    "- By predicting the next token well across billions of examples, the model learns grammar, semantics, style, reasoning patterns, and world knowledge.  \n",
    "- All of this comes from one simple principle: *minimize negative log-likelihood (maximize the probability of real text).*\n",
    "\n",
    "---\n",
    "\n",
    "**Next (Block 1.3):**  \n",
    "We extend this objective to **batches of sequences** (with different lengths), introduce **padding**, and show how to compute a **masked mean token loss** that ignores `<pad>` tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23429d",
   "metadata": {},
   "source": [
    "## 3) Batching, Padding, and the Masked Token Loss\n",
    "\n",
    "So far we looked at single sequences. But in practice, models are trained with **batches** of sequences in parallel, for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Why batching?\n",
    "\n",
    "- GPUs/TPUs can process many examples at once.  \n",
    "- Instead of updating weights after every single sequence, we group **B sequences** into a **batch**.  \n",
    "- This allows **vectorized operations** and faster convergence (by averaging gradients).\n",
    "\n",
    "Formally:  \n",
    "A batch $\\mathcal{B}$ is a set of $B$ token sequences:\n",
    "$$\n",
    "\\mathcal{B} = \\{\\mathbf{t}^{(1)}, \\mathbf{t}^{(2)}, \\dots, \\mathbf{t}^{(B)}\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 The problem of variable lengths\n",
    "\n",
    "Different sequences have different lengths:\n",
    "- Example: `\"The cat\"` → 2 tokens.  \n",
    "- Example: `\"A very long sentence here\"` → 6 tokens.  \n",
    "\n",
    "Neural networks expect tensors with the **same shape**, so we cannot stack them directly.  \n",
    "Solution: **Padding**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Padding sequences\n",
    "\n",
    "Let $L_{\\max}$ be the length of the longest sequence in the batch.  \n",
    "We build a matrix:\n",
    "\n",
    "$$\n",
    "T \\in \\mathbb{N}^{B \\times L_{\\max}}, \\quad\n",
    "T_{b,i} =\n",
    "\\begin{cases}\n",
    "t^{(b)}_i & \\text{if } i \\leq L(\\mathbf{t}^{(b)}), \\\\\\\\\n",
    "\\texttt{<pad>} & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Each row = one sequence.  \n",
    "- Shorter sequences are padded with a special `<pad>` token until they reach length $L_{\\max}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Mask of valid tokens\n",
    "\n",
    "To ensure padding does not affect the loss, we build a **mask**:\n",
    "\n",
    "$$\n",
    "M \\in \\{0,1\\}^{B \\times L_{\\max}}, \\qquad\n",
    "M_{b,i} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } T_{b,i} \\neq \\texttt{<pad>} \\quad (\\text{valid token}), \\\\\\\\\n",
    "0 & \\text{if } T_{b,i} = \\texttt{<pad>} \\quad (\\text{ignored}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Masked mean token loss\n",
    "\n",
    "Without masking, the loss would include `<pad>` tokens, corrupting training.  \n",
    "Instead we average only over **valid tokens**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{token}} =\n",
    "- \\frac{1}{N_{\\text{valid}}}\n",
    "\\sum_{b=1}^B \\sum_{i=1}^{L_{\\max}}\n",
    "M_{b,i} \\; \\log p_\\theta\\!\\big(T_{b,i} \\mid T_{b,<i}\\big),\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "N_{\\text{valid}} = \\sum_{b=1}^B \\sum_{i=1}^{L_{\\max}} M_{b,i}.\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "- Loss is **independent of how much padding there is**.  \n",
    "- Gradients reflect only *real tokens*.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.6 Implementation intuition\n",
    "\n",
    "Most deep learning frameworks implement this using:\n",
    "- A **CrossEntropyLoss** with `ignore_index=pad_id` (in PyTorch).  \n",
    "- An **attention_mask** to indicate valid tokens (in Transformers).\n",
    "\n",
    "Thus:\n",
    "- Forward pass uses the padded tensor.  \n",
    "- Loss computation ignores `<pad>`.  \n",
    "- Attention layers also ignore `<pad>` (via masking).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([3, 6])\n",
      "Mask:\n",
      " tensor([[1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1]])\n",
      "Targets:\n",
      " tensor([[ 5,  6,  7,  8,  0,  0],\n",
      "        [ 9, 10, 11,  0,  0,  0],\n",
      "        [12, 13, 14, 15, 16, 17]])\n",
      "Masked loss: 3.2531940937042236\n",
      "Unmasked loss (PAD counted): 3.3142731189727783\n",
      "Masked loss   (PAD ignored): 3.2531940937042236\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose we have 3 sequences of different lengths\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],      # length 4\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id], # length 3\n",
    "    [12, 13, 14, 15, 16, 17]            # length 6 (max)\n",
    "])\n",
    "\n",
    "print(\"Batch shape:\", batch.shape)  # (3, 6)\n",
    "\n",
    "# Mask: 1 for valid tokens, 0 for pad\n",
    "mask = (batch != pad_id).long()\n",
    "print(\"Mask:\\n\", mask)\n",
    "\n",
    "\n",
    "\n",
    "# Example: \"the cat sleeps\" \n",
    "# # We add special tokens: [BOS] at the start, [EOS] at the end. # Sequence becomes: [BOS], \"the\", \"cat\", \"sleeps\", [EOS] \n",
    "# # The model output (logits) has shape (B, L, V): \n",
    "# # - B = batch size (here 1 sentence) \n",
    "# # - L = sequence length (5 tokens including BOS/EOS) \n",
    "# # - V = vocabulary size \n",
    "#  At each position 'Pos': \n",
    "# # - Pos 1: Context = [BOS], model predicts the first real token (\"the\") \n",
    "# # - Pos 2: Context = [BOS, \"the\"], model predicts \"cat\" #\n",
    "# # - Pos 3: Context = [BOS, \"the\", \"cat\"], model predicts \"sleeps\" \n",
    "# # - Pos 4: Context = [BOS, \"the\", \"cat\", \"sleeps\"], model predicts [EOS] \n",
    "# Each row logits[0, Pos, :] is a vector of size V with scores \n",
    "# # (before softmax) for all tokens in the vocabulary. # After applying softmax, we get a probability distribution # P(next_token | context). \n",
    "# # # In training, we compare this distribution with the true target token # at that position using cross-entropy loss.\n",
    "\n",
    "# Example: logits from a model (random for demo)\n",
    "B, L, V = batch.shape[0], batch.shape[1], 20  # vocab size = 20\n",
    "logits = torch.randn(B, L, V)  # model output\n",
    "\n",
    "# Print this line if you want to inspect the logits tensor.\n",
    "# Shape: (B, L, V)\n",
    "#   - B = number of sequences in the batch\n",
    "#   - L = sequence length (positions/tokens)\n",
    "#   - V = vocabulary size\n",
    "# For each sequence, the model output has shape (L, V):\n",
    "#   - L = sequence length (positions in the sentence)\n",
    "#   - V = vocabulary size\n",
    "# You can see this as an L×V matrix:\n",
    "#   - Each row = a vector of logits over the entire vocabulary \n",
    "#     for one specific position in the sequence.\n",
    "#   - Each column = the score assigned to one fixed vocabulary token \n",
    "#     across all positions.\n",
    "# In practice, we usually interpret the output row by row (position by position),\n",
    "# since after applying softmax, each row becomes a probability distribution \n",
    "# over the vocabulary for the next token at that position.\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Logits:\\n\", logits)\n",
    "\n",
    "\n",
    "\n",
    "# Loss function that ignores pad tokens\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "# Shift targets by one position for next-token prediction\n",
    "targets = batch.clone()\n",
    "targets[targets == pad_id] = pad_id  # keep pad as pad\n",
    "print(\"Targets:\\n\", targets)\n",
    "\n",
    "# Flatten for CE loss\n",
    "loss = loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "print(\"Masked loss:\", loss.item())\n",
    "\n",
    "# Without ignore_index, PAD tokens are treated as real targets:\n",
    "# - Loss & gradients get biased toward predicting PAD\n",
    "# - Shorter sequences contribute many PAD positions\n",
    "# - Metrics (loss/perplexity) become misleading\n",
    "unmasked_loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")  # no ignore_index\n",
    "unmasked_loss = unmasked_loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "\n",
    "print(\"Unmasked loss (PAD counted):\", unmasked_loss.item())\n",
    "print(\"Masked loss   (PAD ignored):\", loss.item())\n",
    "\n",
    "# Rule of thumb:\n",
    "# If your batch contains padding, the unmasked loss is typically larger\n",
    "# and will push the model to predict PAD more often—hurting learning.\n",
    "# Always ignore PAD in the loss (via ignore_index) and in attention masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7eba7d",
   "metadata": {},
   "source": [
    "## 4) Perplexity: measuring how well a model predicts\n",
    "\n",
    "Up to now we focused on the **loss** (negative log-likelihood). But in language modeling, people often report **perplexity (PPL)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Definition\n",
    "\n",
    "For a dataset of $N_{\\text{valid}}$ valid tokens, perplexity is:\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\!\\Bigg( \\frac{1}{N_{\\text{valid}}}\n",
    "\\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})}\n",
    "-\\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}) \\Bigg).\n",
    "$$\n",
    "\n",
    "This is simply the exponential of the **average negative log-likelihood per token**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Intuition\n",
    "\n",
    "- If $\\mathrm{PPL}=1$: the model is perfect — it always assigns probability 1 to the correct token.  \n",
    "- If $\\mathrm{PPL}=V$ (vocabulary size): the model is as bad as random guessing.  \n",
    "- Lower perplexity = better model.\n",
    "\n",
    "In words: **perplexity measures “how many tokens the model is confused among, on average.”**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Why use perplexity?\n",
    "\n",
    "- It is easier to interpret than raw log-loss.  \n",
    "- It is standard in language modeling benchmarks (e.g., Penn Treebank, WikiText).  \n",
    "- It allows fair comparison across models trained with different vocabularies (to some extent).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Example with code\n",
    "Now let’s compute perplexity from the loss in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL per token: 3.4860159067007213\n",
      "Perplexity: 32.6555852921121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Suppose we have predictions for a batch\n",
    "pad_id = 0\n",
    "targets = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id],\n",
    "    [12, 13, 14, 15, 16, 17]\n",
    "])\n",
    "\n",
    "B, L = targets.shape\n",
    "V = 20\n",
    "logits = torch.randn(B, L, V)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss function that ignores padding\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id, reduction=\"sum\")\n",
    "\n",
    "# Compute total loss (sum over tokens, not mean yet)\n",
    "loss_sum = loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "\n",
    "# Count valid tokens (non-pad)\n",
    "valid_tokens = (targets != pad_id).sum().item()\n",
    "\n",
    "# Average NLL per token\n",
    "nll_avg = loss_sum.item() / valid_tokens\n",
    "\n",
    "# Perplexity\n",
    "ppl = math.exp(nll_avg)\n",
    "\n",
    "print(\"Average NLL per token:\", nll_avg)\n",
    "print(\"Perplexity:\", ppl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6e9b6",
   "metadata": {},
   "source": [
    "## 5) Attention Masking: Causal, Padding, and Head Masks\n",
    "\n",
    "So far we focused on **token probabilities** and **loss functions**.  \n",
    "Now we connect this theory to the **attention mechanism** inside Transformers — the core building block of LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 Self-attention in a nutshell\n",
    "\n",
    "In self-attention, every token can attend to every other token.  \n",
    "For a sequence of length $L$ and embedding size $d$, we compute:\n",
    "\n",
    "- **Queries:** $Q \\in \\mathbb{R}^{L \\times d}$  \n",
    "- **Keys:** $K \\in \\mathbb{R}^{L \\times d}$  \n",
    "- **Values:** $V \\in \\mathbb{R}^{L \\times d}$  \n",
    "\n",
    "The **attention scores** are:\n",
    "\n",
    "$$\n",
    "S = \\frac{QK^\\top}{\\sqrt{d}} \\quad \\in \\mathbb{R}^{L \\times L}.\n",
    "$$\n",
    "\n",
    "After applying softmax row-wise, we get the **attention weights**:\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}(S),\n",
    "$$\n",
    "\n",
    "and then the output is:\n",
    "\n",
    "$$\n",
    "O = A \\, V.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 The problem\n",
    "\n",
    "- Without restrictions, a token at position $i$ could attend to **future tokens** $j>i$.  \n",
    "- `<pad>` tokens could receive attention, contaminating representations.  \n",
    "- Sometimes we want to **disable certain heads** for ablation or interpretability.\n",
    "\n",
    "**Solution:** use **masks** that tell the model where it is allowed to attend.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Causal mask (autoregression)\n",
    "\n",
    "**Goal:** prevent a token from looking into the future.\n",
    "\n",
    "Definition:\n",
    "\n",
    "$$\n",
    "C \\in \\{0,1\\}^{L \\times L}, \\qquad\n",
    "C_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } j>i \\quad (\\text{future blocked}), \\\\\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Applied to scores:\n",
    "\n",
    "$$\n",
    "S \\;\\leftarrow\\; S + (-10^9)\\cdot C.\n",
    "$$\n",
    "\n",
    "This sets future positions to a very large negative value (≈$-\\infty$), so after softmax they get probability 0.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Padding mask\n",
    "\n",
    "**Goal:** ignore `<pad>` tokens added during batching.\n",
    "\n",
    "Definition for a batch of size $B$:\n",
    "\n",
    "$$\n",
    "P \\in \\{0,1\\}^{B \\times L}, \\qquad\n",
    "P_{b,j} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if token $j$ in sequence $b$ is <pad>}, \\\\\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "When broadcasted into attention scores, positions marked with 1 are also masked out.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Head mask\n",
    "\n",
    "**Goal:** enable or disable specific heads.\n",
    "\n",
    "For $H$ attention heads:\n",
    "\n",
    "$$\n",
    "h \\in \\{0,1\\}^H \\quad \\text{(or } h \\in \\{0,1\\}^{B \\times H}\\text{ if per-example)}.\n",
    "$$\n",
    "\n",
    "- $h_h=0$ → that head is disabled (all its contributions zeroed).  \n",
    "- Useful for **interpretability** (probing which heads matter), or for **structured pruning**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.6 Combined masking\n",
    "\n",
    "For a full batch, the final scores are:\n",
    "\n",
    "$$\n",
    "S = \\frac{QK^\\top}{\\sqrt{d}}.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Broadcast $C$ (causal mask) to $(1,1,L,L)$.  \n",
    "2. Broadcast $P$ (padding mask) to $(B,1,1,L)$.  \n",
    "3. Combine:  \n",
    "   $$\n",
    "   M = \\text{broadcast}(C) \\;\\lor\\; \\text{broadcast}(P).\n",
    "   $$\n",
    "4. Apply to scores:  \n",
    "   $$\n",
    "   S \\;\\leftarrow\\; S + (-10^9)\\cdot M.\n",
    "   $$\n",
    "5. Softmax row-wise over keys:  \n",
    "   $$\n",
    "   A = \\text{softmax}(S).\n",
    "   $$\n",
    "6. Apply head mask (if any):  \n",
    "   $$\n",
    "   A \\;\\leftarrow\\; A \\odot \\text{broadcast}(h).\n",
    "   $$\n",
    "7. Final output:  \n",
    "   $$\n",
    "   O = A \\, V.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.7 Why attention masking is essential\n",
    "\n",
    "- **Causal mask:** enforces the autoregressive factorization (no cheating by looking at the future).  \n",
    "- **Padding mask:** ensures that `<pad>` tokens do not leak into the computation.  \n",
    "- **Head mask:** gives flexibility for analysis and pruning.\n",
    "\n",
    "Together, these masks align the **mathematics of training (loss + padding)** with the **architecture of attention**.\n",
    "\n",
    "---\n",
    "\n",
    "**Next (Block 1.6):**  \n",
    "We will implement these masks in **PyTorch code**:  \n",
    "1. Build a causal mask matrix.  \n",
    "2. Build a padding mask from a batch with `<pad>`.  \n",
    "3. Show how they are applied inside a MultiheadAttention layer.  \n",
    "4. Visualize the causal mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797ff7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask:\n",
      " tensor([[0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "Padding mask shape: torch.Size([3, 6])\n",
      "Output shape: torch.Size([3, 6, 16])\n",
      "Attention weights shape: torch.Size([3, 6, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FLEX\\LLM\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Create toy batch (with padding)\n",
    "# -------------------------------\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],      # length 4\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id], # length 3\n",
    "    [12, 13, 14, 15, 16, 17]            # length 6 (max)\n",
    "])\n",
    "\n",
    "B, L = batch.shape\n",
    "D = 16   # embedding dimension\n",
    "H = 2    # number of heads\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build causal mask\n",
    "# -------------------------------\n",
    "# Shape: (L, L). We put -inf above the main diagonal so that\n",
    "# position i cannot attend to positions j > i (future tokens).\n",
    "# This mask is ADDED to attention scores before softmax:\n",
    "#   scores' = scores + mask\n",
    "# and exp(-inf) -> 0, so future gets zero probability.\n",
    "causal_mask = torch.triu(torch.ones(L, L) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "print(\"Causal mask:\\n\", causal_mask)\n",
    "\n",
    "# Causal mask math (why it blocks future tokens):\n",
    "# \n",
    "# Self-attention computes scores S = (Q K^T) / sqrt(d_k), shape (L, L).\n",
    "# We add a mask M with:\n",
    "#   M[i, j] = 0       if j <= i   (past + self allowed)\n",
    "#   M[i, j] = -inf    if j >  i   (future blocked)\n",
    "# so the masked scores are S' = S + M.\n",
    "#\n",
    "# Attention weights are a row-wise softmax:\n",
    "#   alpha[i, j] = exp(S'[i, j]) / sum_k exp(S'[i, k])\n",
    "# For any future position j > i, S'[i, j] = S[i, j] + (-inf) = -inf,\n",
    "# hence exp(S'[i, j]) = 0  ->  alpha[i, j] = 0.\n",
    "# Therefore the token at position i cannot attend to any future token.\n",
    "#\n",
    "# The output at position i is:\n",
    "#   out[i] = sum_j alpha[i, j] * V[j]\n",
    "# Since alpha[i, j] = 0 for all j > i, only past and self (j <= i) contribute.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Build key-padding mask\n",
    "# -------------------------------\n",
    "# True where PAD tokens are present\n",
    "padding_mask = (batch == pad_id)\n",
    "print(\"Padding mask shape:\", padding_mask.shape)  # (B, L)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. MultiheadAttention with masks\n",
    "# -------------------------------\n",
    "# IMPORTANT: x below are EMBEDDINGS (B, L, D), not logits.\n",
    "# In a real model, x would come from nn.Embedding(V,D) (+ positional encodings).\n",
    "attn = nn.MultiheadAttention(embed_dim=D, num_heads=H, batch_first=True)\n",
    "\n",
    "# Random embeddings to demo the mechanics (these are NOT probabilities).\n",
    "x = torch.randn(B, L, D)\n",
    "\n",
    "# Apply self-attention with both masks:\n",
    "# - attn_mask (L, L) blocks FUTURE positions (causal)\n",
    "# - key_padding_mask (B, L) prevents attending to PAD tokens\n",
    "out, attn_weights = attn(\n",
    "    x, x, x,\n",
    "    attn_mask=causal_mask,         # float mask: 0 allowed, -inf blocked\n",
    "    key_padding_mask=padding_mask  # bool mask: True = PAD = block as K/V\n",
    ")\n",
    "\n",
    "print(\"Output shape:\", out.shape)  # (B, L, D) -> contextualized embeddings\n",
    "\n",
    "# About attn_weights shape:\n",
    "#   - Some PyTorch versions return (B, L, L) (averaged over heads).\n",
    "#   - Others may return (B*H, L, L).\n",
    "# If you want per-head weights explicitly, set average_attn_weights=False:\n",
    "#   out, attn_weights = attn(x, x, x, attn_mask=causal_mask,\n",
    "#                            key_padding_mask=padding_mask,\n",
    "#                            average_attn_weights=False)\n",
    "#   -> attn_weights shape: (B, H, L, L)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Visualize causal mask\n",
    "# -------------------------------\n",
    "plt.imshow(causal_mask.numpy(), cmap=\"gray\")\n",
    "plt.title(\"Causal Attention Mask (white = -inf)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Notes:\n",
    "# - D (embedding dim) is the model's feature space; V (vocab size) is NOT used here.\n",
    "#   This cell is about attention over embeddings, not about vocabulary logits.\n",
    "# - Keep all tensors on the same device in real training (e.g., causal_mask = causal_mask.to(x.device)).\n",
    "# - Use both masks during training: causal (no future) + key_padding (no PAD).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf020b",
   "metadata": {},
   "source": [
    "### 6.5 Complete attention implementation with fundamental masking\n",
    "\n",
    "**Essential mathematical components:**  \n",
    "We now implement a **minimal but complete** attention mechanism that demonstrates the core masking principles from our mathematical foundations.\n",
    "\n",
    "**Key requirements:**\n",
    "1. **Causal masking**: Enforce $p(t_i | t_{<i})$ by preventing future access\n",
    "2. **Padding masking**: Ignore `<pad>` tokens during attention computation  \n",
    "3. **Correct softmax normalization**: Ensure $\\sum_{j \\in \\text{valid}} A_{ij} = 1$\n",
    "\n",
    "**Implementation principle:**  \n",
    "Apply masks to attention scores **before softmax**, then verify that the mathematical properties we derived are satisfied in practice.\n",
    "\n",
    "$$\n",
    "\\text{scores}_{\\text{masked}} = \\text{scores}_{\\text{raw}} + \\text{causal\\_mask} + \\text{padding\\_mask}\n",
    "$$\n",
    "$$\n",
    "A = \\text{softmax}(\\text{scores}_{\\text{masked}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57e81ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple attention output shape: torch.Size([3, 6, 8])\n",
      "Attention weights shape: torch.Size([3, 6, 6])\n",
      "\n",
      "Verification - attention sums for first sequence:\n",
      "  Query 0: sum over valid keys = 1.000000\n",
      "  Query 1: sum over valid keys = 1.000000\n",
      "  Query 2: sum over valid keys = 1.000000\n",
      "  Query 3: sum over valid keys = 1.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  # <- needed for F.softmax\n",
    "\n",
    "def simple_masked_attention(embeddings, causal_mask=None, key_padding_mask=None):\n",
    "    \"\"\"\n",
    "    Minimal implementation demonstrating core masking principles.\n",
    "\n",
    "    Args:\n",
    "        embeddings: (B, L, d) input embeddings (float)\n",
    "        causal_mask: (L, L) float mask with 0 on allowed, -inf on blocked (future)\n",
    "        key_padding_mask: (B, L) bool mask, True where positions are PAD (block as keys)\n",
    "\n",
    "    Returns:\n",
    "        output: (B, L, d) attention output (contextualized embeddings)\n",
    "        attention_weights: (B, L, L) row-wise distributions over keys\n",
    "    \"\"\"\n",
    "    B, L, d = embeddings.shape\n",
    "\n",
    "    # Q=K=V for self-attention demo (same space, three roles)\n",
    "    Q = embeddings\n",
    "    K = embeddings\n",
    "    V = embeddings\n",
    "\n",
    "    # Scores: (B, L, L) — query i against all keys j\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d ** 0.5)\n",
    "\n",
    "    # Causal mask: block future (broadcast (L,L) -> (B,L,L))\n",
    "    if causal_mask is not None:\n",
    "        # ensure same device/dtype: float mask (0 or -inf)\n",
    "        scores = scores + causal_mask.to(scores.dtype).to(scores.device).unsqueeze(0)\n",
    "\n",
    "    # Key padding mask: block PAD columns (broadcast (B,L) -> (B,L,L))\n",
    "    if key_padding_mask is not None:\n",
    "        # True means \"this key j is PAD\" → set scores[:,:,j] = -inf\n",
    "        expanded_mask = key_padding_mask.to(torch.bool).to(scores.device).unsqueeze(1).expand(-1, L, -1)\n",
    "        scores = scores.masked_fill(expanded_mask, float('-inf'))\n",
    "\n",
    "    # Row-wise softmax → attention weights α (each row sums to 1 over unmasked keys)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Weighted sum of values → contextualized outputs\n",
    "    output = torch.matmul(attention_weights, V)  # (B, L, d)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# ---- Test with your batch/masks ---------------------------------------------\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5,  6,  7,  8,  pad_id, pad_id],\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id],\n",
    "    [12,13, 14, 15, 16,     17]\n",
    "])\n",
    "B, L = batch.shape\n",
    "d_embed = 8\n",
    "\n",
    "# Embeddings demo (random): (B, L, d)\n",
    "embeddings = torch.randn(B, L, d_embed)\n",
    "\n",
    "# Causal mask (L,L): 0 on past/self, -inf on future\n",
    "causal_mask = torch.triu(torch.ones(L, L) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "# Key padding mask (B,L): True where PAD\n",
    "key_padding_mask = (batch == pad_id)\n",
    "\n",
    "# Run simple attention\n",
    "output, attn_weights = simple_masked_attention(\n",
    "    embeddings, causal_mask, key_padding_mask\n",
    ")\n",
    "\n",
    "print(f\"Simple attention output shape: {output.shape}\")      # (B, L, d)\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")      # (B, L, L)\n",
    "\n",
    "# ---- Quick verification: row-sum over valid keys is ~1 -----------------------\n",
    "print(\"\\nVerification - attention sums for first sequence:\")\n",
    "for i in range(min(L, 4)):\n",
    "    valid_keys = (~key_padding_mask[0]) & (torch.arange(L) <= i)\n",
    "    attn_sum = attn_weights[0, i, valid_keys].sum().item()\n",
    "    print(f\"  Query {i}: sum over valid keys = {attn_sum:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3c097",
   "metadata": {},
   "source": [
    "### 6.6 Visualizing attention masks and patterns\n",
    "\n",
    "**Visualization objectives:**\n",
    "1. **Understand mask structure**: See how causal and padding constraints look\n",
    "2. **Verify mask application**: Confirm that attention respects our constraints  \n",
    "3. **Interpret attention patterns**: Connect math to intuitive understanding\n",
    "\n",
    "**What to look for:**\n",
    "- **Lower triangular pattern**: Causal masking creates this structure\n",
    "- **Zero attention to padding**: Padded positions should receive no attention\n",
    "- **Probability conservation**: Each row sums to 1 over valid positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeca90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_attention_fundamentals(attention_weights, batch_tokens, causal_mask, key_padding_mask, pad_id):\n",
    "    \"\"\"\n",
    "    Create fundamental visualizations for understanding attention masking.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot 1: Causal mask structure\n",
    "    axes[0,0].imshow(causal_mask.numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[0,0].set_title('Causal Mask\\n(white = blocked, blue = allowed)')\n",
    "    axes[0,0].set_xlabel('Key positions')\n",
    "    axes[0,0].set_ylabel('Query positions')\n",
    "    \n",
    "    # Plot 2: Key padding mask for first sequence\n",
    "    pad_viz = key_padding_mask[0].float().unsqueeze(0).numpy()\n",
    "    axes[0,1].imshow(pad_viz, cmap='RdBu_r', aspect='auto')\n",
    "    axes[0,1].set_title(f'Key Padding Mask (Seq 0)\\n(red = padding)')\n",
    "    axes[0,1].set_xlabel('Key positions')\n",
    "    axes[0,1].set_yticks([])\n",
    "    \n",
    "    # Add token labels\n",
    "    tokens = batch_tokens[0].numpy()\n",
    "    labels = []\n",
    "    for token_id in tokens:\n",
    "        if token_id == pad_id:\n",
    "            labels.append('PAD')\n",
    "        elif token_id == BOS_ID:\n",
    "            labels.append('BOS')\n",
    "        elif token_id == EOS_ID:\n",
    "            labels.append('EOS')\n",
    "        else:\n",
    "            labels.append(f't{token_id}')\n",
    "    \n",
    "    axes[0,1].set_xticks(range(len(labels)))\n",
    "    axes[0,1].set_xticklabels(labels, rotation=45)\n",
    "    \n",
    "    # Plot 3: Actual attention weights (first sequence)\n",
    "    attn_viz = attention_weights[0].numpy()\n",
    "    im3 = axes[1,0].imshow(attn_viz, cmap='Blues', vmin=0, vmax=attn_viz.max())\n",
    "    axes[1,0].set_title('Attention Weights (Seq 0)\\nafter masking')\n",
    "    axes[1,0].set_xlabel('Key positions')\n",
    "    axes[1,0].set_ylabel('Query positions')\n",
    "    axes[1,0].set_xticks(range(len(labels)))\n",
    "    axes[1,0].set_xticklabels(labels, rotation=45)\n",
    "    axes[1,0].set_yticks(range(len(labels)))\n",
    "    axes[1,0].set_yticklabels(labels)\n",
    "    plt.colorbar(im3, ax=axes[1,0], fraction=0.046)\n",
    "    \n",
    "    # Plot 4: Attention distribution for a specific query\n",
    "    query_idx = 3  # Look at query position 3\n",
    "    valid_keys = ~key_padding_mask[0] & (torch.arange(L) <= query_idx)\n",
    "    \n",
    "    axes[1,1].bar(range(L), attention_weights[0, query_idx].numpy(), alpha=0.7)\n",
    "    axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Highlight valid vs invalid positions\n",
    "    for i, (is_valid, label) in enumerate(zip(valid_keys, labels)):\n",
    "        color = 'green' if is_valid else 'red'\n",
    "        axes[1,1].axvline(x=i, color=color, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    axes[1,1].set_title(f'Attention Distribution\\nQuery position {query_idx}')\n",
    "    axes[1,1].set_xlabel('Key positions')\n",
    "    axes[1,1].set_ylabel('Attention weight')\n",
    "    axes[1,1].set_xticks(range(len(labels)))\n",
    "    axes[1,1].set_xticklabels(labels, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical verification\n",
    "    print(\"=== Numerical Verification ===\")\n",
    "    print(f\"Query {query_idx} attention sum: {attention_weights[0, query_idx, valid_keys].sum():.6f}\")\n",
    "    print(f\"Attention to future positions: {attention_weights[0, query_idx, query_idx+1:].sum():.8f}\")\n",
    "    print(f\"Attention to padding: {attention_weights[0, query_idx, key_padding_mask[0]].sum():.8f}\")\n",
    "\n",
    "# Create the visualization\n",
    "visualize_attention_fundamentals(attn_weights, batch_tokens, causal_mask, key_padding_mask, PAD_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dd174",
   "metadata": {},
   "source": [
    "### 6.7 Practical exercises with the tiny corpus\n",
    "\n",
    "Now we apply our masking concepts to the **tiny corpus** from the data folder. This bridges theory with practice using real tokenized text.\n",
    "\n",
    "**Learning objectives:**\n",
    "1. **Tokenize real text** and apply masking\n",
    "2. **Compute masked losses** on actual sequences  \n",
    "3. **Observe attention patterns** on linguistic content\n",
    "4. **Verify mathematical properties** hold for real data\n",
    "\n",
    "**Tiny corpus content:**\n",
    "- `\"the quick brown fox jumps over the lazy dog\"`\n",
    "- `\"abracadabra abracadabra token tokenization\"`  \n",
    "- `\"subword models like bpe and unigram are common\"`\n",
    "\n",
    "This gives us realistic sequence lengths and repeated tokens to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the tiny corpus\n",
    "def load_tiny_corpus():\n",
    "    \"\"\"Load the tiny corpus and create a simple tokenizer.\"\"\"\n",
    "    \n",
    "    # Read the tiny corpus (from the data/ folder)\n",
    "    corpus_text = \"\"\"the quick brown fox jumps over the lazy dog\n",
    "abracadabra abracadabra token tokenization\n",
    "subword models like bpe and unigram are common\"\"\"\n",
    "    \n",
    "    # Simple word-level tokenization\n",
    "    sentences = corpus_text.strip().split('\\n')\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab = {'<pad>': 0, '<bos>': 1, '<eos>': 2}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    tokenized = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [vocab['<bos>']]\n",
    "        tokens.extend(vocab[word] for word in sentence.split())\n",
    "        tokens.append(vocab['<eos>'])\n",
    "        tokenized.append(tokens)\n",
    "    \n",
    "    return tokenized, vocab\n",
    "\n",
    "# Process the corpus\n",
    "tokenized_corpus, vocab = load_tiny_corpus()\n",
    "vocab_size = len(vocab)\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(\"=== Tiny Corpus Analysis ===\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {list(vocab.keys())}\")\n",
    "print(\"\\nTokenized sentences:\")\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    words = [id_to_token[token_id] for token_id in tokens]\n",
    "    print(f\"  {i}: {tokens} -> {words}\")\n",
    "\n",
    "# Create a padded batch from the corpus\n",
    "max_len = max(len(seq) for seq in tokenized_corpus)\n",
    "corpus_batch = torch.zeros(len(tokenized_corpus), max_len, dtype=torch.long)\n",
    "\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    corpus_batch[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    # Remaining positions are already 0 (PAD_ID)\n",
    "\n",
    "print(f\"\\nCorpus batch shape: {corpus_batch.shape}\")\n",
    "print(\"Corpus batch:\")\n",
    "print(corpus_batch)\n",
    "\n",
    "# Create masks for the corpus\n",
    "corpus_causal_mask = create_causal_mask(max_len)\n",
    "corpus_padding_mask = create_key_padding_mask(corpus_batch, PAD_ID)\n",
    "\n",
    "print(f\"\\nMask shapes:\")\n",
    "print(f\"  Causal: {corpus_causal_mask.shape}\")\n",
    "print(f\"  Padding: {corpus_padding_mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35b7f3",
   "metadata": {},
   "source": [
    "### 6.8 Block 1 Summary and Transition\n",
    "\n",
    "**What we accomplished in Block 1:**\n",
    "\n",
    "**Mathematical Foundations** ✅\n",
    "- Formalized tokenization: $\\tau: \\Sigma^* \\to \\mathcal{V}^*$\n",
    "- Derived autoregressive factorization: $p_\\theta(x) = \\prod_{i=1}^L p_\\theta(t_i | t_{<i})$\n",
    "- Connected MLE to cross-entropy loss: $\\mathcal{L} = -\\sum \\log p_\\theta(t_i | t_{<i})$\n",
    "\n",
    "**Batching Mathematics** ✅  \n",
    "- Formalized padding and masked token loss\n",
    "- Proved padding invariance with proper masking\n",
    "- Computed perplexity as $\\exp(\\text{mean NLL})$\n",
    "\n",
    "**Attention Masking Theory** ✅\n",
    "- Proved necessity of causal masking for autoregressive consistency\n",
    "- Derived key-padding mask for variable-length sequences\n",
    "- Implemented and verified mask composition\n",
    "\n",
    "**Key Mathematical Insights:**\n",
    "1. **Masking is not optional** - it's required for mathematical consistency\n",
    "2. **Padding creates measure-zero artifacts** that must be excluded from loss\n",
    "3. **Attention weights form probability simplexes** over valid positions only\n",
    "4. **Causal constraints preserve information-theoretic bottleneck** properties\n",
    "\n",
    "---\n",
    "\n",
    "**Transition to Block 2: Transformer Architecture**\n",
    "\n",
    "Block 2 will build on these foundations to explore:\n",
    "- **Complete transformer layers** (LayerNorm, residuals, FFN)\n",
    "- **Positional encodings** and their mathematical properties  \n",
    "- **Multi-layer composition** and representation learning theory\n",
    "- **Implementation from scratch** with comprehensive analysis\n",
    "\n",
    "The mathematical rigor established here provides the foundation for understanding **why** transformers work, not just **how** they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd99d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: compute masked loss on tiny corpus\n",
    "def compute_corpus_loss(batch_tokens, vocab_size, causal_mask, padding_mask):\n",
    "    \"\"\"\n",
    "    Compute the masked language modeling loss on our tiny corpus.\n",
    "    This demonstrates the complete pipeline from Block 1.\n",
    "    \"\"\"\n",
    "    B, L = batch_tokens.shape\n",
    "    \n",
    "    # Create dummy \"model predictions\" (random logits)\n",
    "    # In reality, these would come from a transformer\n",
    "    logits = torch.randn(B, L, vocab_size) * 0.1\n",
    "    \n",
    "    # Targets for next-token prediction: shift input by 1\n",
    "    targets = batch_tokens.clone()\n",
    "    \n",
    "    # Compute cross-entropy loss with padding ignored\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, reduction='none')\n",
    "    \n",
    "    # Reshape for loss computation\n",
    "    logits_flat = logits.view(-1, vocab_size)  # (B*L, V)\n",
    "    targets_flat = targets.view(-1)             # (B*L,)\n",
    "    \n",
    "    # Compute per-token losses\n",
    "    token_losses = loss_fn(logits_flat, targets_flat)  # (B*L,)\n",
    "    token_losses = token_losses.view(B, L)             # (B, L)\n",
    "    \n",
    "    # Apply padding mask to exclude PAD tokens from loss\n",
    "    valid_tokens = ~padding_mask\n",
    "    masked_losses = token_losses * valid_tokens.float()\n",
    "    \n",
    "    # Compute mean loss over valid tokens only\n",
    "    total_loss = masked_losses.sum()\n",
    "    total_valid = valid_tokens.sum()\n",
    "    mean_loss = total_loss / total_valid\n",
    "    \n",
    "    # Compute perplexity\n",
    "    perplexity = torch.exp(mean_loss)\n",
    "    \n",
    "    return mean_loss, perplexity, token_losses, valid_tokens\n",
    "\n",
    "# Apply to our corpus\n",
    "corpus_loss, corpus_ppl, token_losses, valid_mask = compute_corpus_loss(\n",
    "    corpus_batch, vocab_size, corpus_causal_mask, corpus_padding_mask\n",
    ")\n",
    "\n",
    "print(\"=== Final Block 1 Demonstration ===\")\n",
    "print(f\"Corpus mean loss: {corpus_loss:.4f}\")\n",
    "print(f\"Corpus perplexity: {corpus_ppl:.4f}\")\n",
    "print(f\"Total valid tokens: {valid_mask.sum().item()}\")\n",
    "\n",
    "print(\"\\nPer-sequence analysis:\")\n",
    "for i in range(len(tokenized_corpus)):\n",
    "    seq_valid = valid_mask[i]\n",
    "    seq_loss = token_losses[i, seq_valid].mean()\n",
    "    seq_ppl = torch.exp(seq_loss)\n",
    "    tokens = [id_to_token[tid] for tid in corpus_batch[i, seq_valid].tolist()]\n",
    "    \n",
    "    print(f\"  Sequence {i}: loss={seq_loss:.4f}, ppl={seq_ppl:.4f}\")\n",
    "    print(f\"             tokens: {tokens}\")\n",
    "\n",
    "print(f\"\\n🎓 Block 1 Complete!\")\n",
    "print(f\"   ✅ Mathematical foundations established\")  \n",
    "print(f\"   ✅ Masking theory and implementation verified\")\n",
    "print(f\"   ✅ Ready for Block 2: Transformer Architecture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
