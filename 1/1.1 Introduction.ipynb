{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff9d60c",
   "metadata": {},
   "source": [
    "# Block 1 — Mathematical Foundations of Tokenization, Batching & Attention Masking\n",
    "\n",
    "This notebook builds the **mathematical backbone** for how LLMs go from raw text to tokens, how batches are formed and losses computed, and how attention masks (causal/padding/head) enforce the same principles architecturally.\n",
    "\n",
    "---\n",
    "\n",
    "## What you will learn in Block 1\n",
    "1. **Objects & notation**: strings, normalization, tokenizer, vocabulary, special tokens.  \n",
    "2. **Autoregressive factorization** of sequence probability.  \n",
    "3. **Maximum-likelihood objective (NLL)** and its link to **cross-entropy**.  \n",
    "4. **Batching with padding** and the **masked mean token loss**.  \n",
    "5. **Perplexity** as exponential of mean NLL.  \n",
    "6. **Attention masking (preview)**: causal mask, key-padding mask, and head masking.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Objects and notation\n",
    "\n",
    "- **Alphabet & strings.**  \n",
    "  Let $\\Sigma$ be the raw character alphabet (e.g., Unicode code points).  \n",
    "  Any finite string is an element:\n",
    "  $$\n",
    "  x \\in \\Sigma^{*}.\n",
    "  $$\n",
    "\n",
    "- **Normalization.**  \n",
    "  A preprocessing map:\n",
    "  $$\n",
    "  \\nu:\\Sigma^{*} \\to \\Sigma^{*}, \\qquad x \\mapsto \\nu(x),\n",
    "  $$\n",
    "  that may include:\n",
    "  - Unicode normalization (e.g., NFKC),  \n",
    "  - case folding (lowercasing),  \n",
    "  - whitespace and punctuation rules.  \n",
    "\n",
    "  *Remark:* Normalization **changes the support** of the data distribution by altering how strings map to tokens (e.g., “Café” → “Cafe”).\n",
    "\n",
    "- **Tokenizer.**  \n",
    "  A **tokenizer** is a function that segments normalized text into discrete units (tokens) from a fixed vocabulary $\\mathcal{V}$.  \n",
    "  Mathematically:\n",
    "  $$\n",
    "  \\tau: \\Sigma^{*} \\to \\mathcal{V}^{*}, \\qquad\n",
    "  \\mathbf{t}=\\tau(\\nu(x))=(t_1,\\dots,t_{L(x)}), \\quad t_i \\in \\mathcal{V}.\n",
    "  $$\n",
    "\n",
    "  \n",
    "  - Tokenizers define the *basic symbols* LLMs see.  \n",
    "  - They can split text into **characters** (character-level), **words** (word-level), or **subwords** (BPE/Unigram).  \n",
    "  - Subword tokenization (e.g., “tokenization” → `[\"token\", \"ization\"]`) balances vocabulary size with sequence length.  \n",
    "  - This mapping is deterministic: the same string always yields the same token sequence.  \n",
    "  - Without tokenization, raw text would be unmanageable: the space of possible strings $\\Sigma^{*}$ is infinite, but the space of tokens $\\mathcal{V}$ is finite and tractable.\n",
    "\n",
    "- **Vocabulary & special tokens.**  \n",
    "  The vocabulary $\\mathcal{V}=\\{0,1,\\dots,V-1\\}$ contains learned tokens plus reserved IDs:\n",
    "  - `<bos>` : beginning of sequence  \n",
    "  - `<eos>` : end of sequence  \n",
    "  - `<pad>` : padding (used in batching)  \n",
    "  - `<unk>` : unknown token (for OOV strings)\n",
    "\n",
    "- **Augmented sequence (optional).**  \n",
    "  To mark boundaries explicitly:\n",
    "  $$\n",
    "  \\tilde{\\mathbf{t}} = (\\texttt{<bos>},\\, t_1,\\dots,t_{L(x)},\\, \\texttt{<eos>})\n",
    "  $$\n",
    "\n",
    "- **Dataset.**  \n",
    "  A dataset is a collection of $N$ independent samples:\n",
    "  $$\n",
    "  \\mathcal{D} = \\{\\, x^{(n)} \\,\\}_{n=1}^{N}.\n",
    "  $$\n",
    "  After tokenization:\n",
    "  $$\n",
    "  \\mathcal{D}_{\\text{tok}} = \\{\\, \\mathbf{t}^{(n)} \\,\\}_{n=1}^{N}, \\qquad\n",
    "  \\mathbf{t}^{(n)} = (t^{(n)}_1, \\dots, t^{(n)}_{L(x^{(n)})}).\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Autoregressive factorization (preview)\n",
    "\n",
    "An LLM with parameters $\\theta$ assigns probability to a tokenized string via the **causal chain rule**:\n",
    "\n",
    "$$\n",
    "p_\\theta(x) = \\prod_{i=1}^{L(x)} p_\\theta\\!\\big(t_i \\mid t_{<i}\\big),\n",
    "\\qquad t_{<i}=(t_1,\\dots,t_{i-1}).\n",
    "$$\n",
    "\n",
    "With explicit boundaries:\n",
    "$$\n",
    "p_\\theta(x) = \\prod_{i=1}^{\\tilde L} p_\\theta\\!\\big(\\tilde t_i \\mid \\tilde t_{<i}\\big),\n",
    "\\quad \\tilde t_1=\\texttt{<bos>},\\ \\tilde t_{\\tilde L}=\\texttt{<eos>}.\n",
    "$$\n",
    "\n",
    "*Interpretation:*  \n",
    "The model decomposes the text into **conditional next-token probabilities**; causality forbids peeking at the future.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training objective (preview)\n",
    "\n",
    "Given dataset $\\mathcal{D}$, **maximum likelihood** maximizes the log-probability assigned to the data:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta}\\; \\sum_{x \\in \\mathcal{D}} \\sum_{i=1}^{L(x)} \\log p_\\theta\\!\\big(t_i \\mid t_{<i}\\big).\n",
    "$$\n",
    "\n",
    "Equivalently, we **minimize the negative log-likelihood (NLL)**.  \n",
    "Later we will connect this to **softmax cross-entropy**, show the **masked mean over valid tokens** (excluding `<pad>`), and define **perplexity**:\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\!\\big(\\text{mean NLL per valid token}\\big).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Attention masking (preview)\n",
    "\n",
    "Self-attention computes scaled dot products between queries and keys.  \n",
    "To respect the mathematics above, we add:\n",
    "- a **causal mask** (no attending to future positions),  \n",
    "- a **key-padding mask** (ignore `<pad>` positions),  \n",
    "- and optionally a **head mask** (enable/disable heads for ablation/interpretability).\n",
    "\n",
    "Later we will show how these masks combine into the attention logits **before** the softmax.\n",
    "\n",
    "---\n",
    "\n",
    "### Outcome of Block 1\n",
    "\n",
    "By the end of Block 1 you will be able to:\n",
    "- Map raw strings $\\to$ tokens with clear assumptions about normalization and special tokens.  \n",
    "- Write and reason about $p_\\theta(x)$ via autoregressive factorization.  \n",
    "- Derive the MLE/NLL objective and relate it to cross-entropy.  \n",
    "- Implement padding-aware **masked mean** losses and compute **perplexity**.  \n",
    "- Explain how **attention masks** (causal/padding/head) enforce the same constraints architecturally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bedfd1",
   "metadata": {},
   "source": [
    "## 2) Autoregressive Factorization and Training Objective\n",
    "\n",
    "Now that we understand what **tokens** are and how a **tokenizer** converts text into discrete units, we ask:  \n",
    "**How does a Large Language Model assign a probability to a whole sequence of tokens?**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 The chain rule of probability\n",
    "\n",
    "From basic probability theory:  \n",
    "For any sequence of random variables $(X_1, X_2, \\dots, X_L)$ we can always write:\n",
    "\n",
    "$$\n",
    "p(X_1, X_2, \\dots, X_L) = \\prod_{i=1}^L p(X_i \\mid X_1, \\dots, X_{i-1}).\n",
    "$$\n",
    "\n",
    "This is called the **chain rule**.  \n",
    "It is an identity, not an assumption — it always holds.\n",
    "\n",
    "- $p(X_1)$ is the probability of the first element.  \n",
    "- $p(X_2 \\mid X_1)$ is the probability of the second given the first.  \n",
    "- $p(X_3 \\mid X_1, X_2)$ is the probability of the third given the first two.  \n",
    "- And so on.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Applying the chain rule to tokens\n",
    "\n",
    "For a token sequence $\\mathbf{t} = (t_1, t_2, \\dots, t_L)$ produced by the tokenizer:\n",
    "\n",
    "$$\n",
    "p_\\theta(\\mathbf{t}) = \\prod_{i=1}^L p_\\theta(t_i \\mid t_1, \\dots, t_{i-1}),\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the model parameters (the weights of the neural network).\n",
    "\n",
    "- $t_i$ is the $i$-th token.  \n",
    "- $t_{<i} = (t_1, \\dots, t_{i-1})$ is the prefix (all tokens before $i$).  \n",
    "- Each factor is the probability of the **next token** given all previous ones.\n",
    "\n",
    "With explicit boundary tokens `<bos>` (begin) and `<eos>` (end):\n",
    "\n",
    "$$\n",
    "p_\\theta(\\tilde{\\mathbf{t}}) = \\prod_{i=1}^{\\tilde L} p_\\theta(\\tilde t_i \\mid \\tilde t_{<i}),\n",
    "$$\n",
    "\n",
    "where $\\tilde L = L + 2$ because of the added boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Why autoregression?\n",
    "\n",
    "- **Causality.** Humans generate text left-to-right; the model imitates this by only looking at the past.  \n",
    "- **Simplicity.** Instead of modeling an entire sequence directly, we only need to model *next-token prediction*.  \n",
    "- **Flexibility.** With this formulation we can generate text token by token: sample $t_1$ from $p(t_1)$, then $t_2$ from $p(t_2\\mid t_1)$, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Training objective: Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "We want our model to assign **high probability** to real text sequences from the dataset.  \n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{ \\mathbf{t}^{(n)} \\}_{n=1}^N$, the **log-likelihood** is:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathcal{D}) = \\sum_{n=1}^N \\log p_\\theta(\\mathbf{t}^{(n)}).\n",
    "$$\n",
    "\n",
    "Using the autoregressive factorization:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathbf{t}^{(n)}) = \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}).\n",
    "$$\n",
    "\n",
    "Thus the training objective is:\n",
    "\n",
    "$$\n",
    "\\max_\\theta \\; \\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta\\!\\big(t^{(n)}_i \\mid t^{(n)}_{<i}\\big).\n",
    "$$\n",
    "\n",
    "This is called **Maximum Likelihood Estimation (MLE).**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 Why logs?\n",
    "\n",
    "If we worked directly with the product $\\prod p(t_i \\mid t_{<i})$, probabilities quickly become astronomically small (multiplying many numbers less than 1).  \n",
    "Taking the logarithm:\n",
    "\n",
    "- Turns products into sums (easier to compute).  \n",
    "- Stabilizes numerics (avoids underflow).  \n",
    "- Matches information-theoretic interpretations (log-loss).  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Negative log-likelihood (NLL)\n",
    "\n",
    "Instead of maximizing log-likelihood, we minimize the **negative log-likelihood**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\theta) = -\\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})} \\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}).\n",
    "$$\n",
    "\n",
    "This is the **loss function** of LLMs.  \n",
    "Minimizing it means: *the model gets penalized whenever it assigns low probability to the correct next token.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.7 Connection to cross-entropy\n",
    "\n",
    "At each position $i$, the model outputs a probability distribution over the vocabulary $\\mathcal{V}$.  \n",
    "\n",
    "If the true token is $t_i$, the per-token loss is:\n",
    "\n",
    "$$\n",
    "\\ell_i = -\\log p_\\theta(t_i \\mid t_{<i}).\n",
    "$$\n",
    "\n",
    "This is exactly the **cross-entropy** between the predicted distribution $p_\\theta(\\cdot \\mid t_{<i})$ and the true one-hot distribution $y$ where $y_{k} = 1$ if $k=t_i$:\n",
    "\n",
    "$$\n",
    "\\ell_i = H(y, p_\\theta) = - \\sum_{k=1}^{V} y_k \\log p_\\theta(k \\mid t_{<i}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.8 Gradient identity (intuition for learning)\n",
    "\n",
    "The derivative of the cross-entropy with respect to the logits $z_{i,k}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_{i,k}} = p_\\theta(k \\mid t_{<i}) - y_k.\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- If the model assigns too much probability to the wrong token, the gradient pushes it down.  \n",
    "- If the model assigns too little to the correct token, the gradient pushes it up.  \n",
    "- Learning is simply **adjusting logits so that predicted probabilities match observed tokens.**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.9 Intuition\n",
    "\n",
    "- Training an LLM = **teaching it to be a good next-token predictor**.  \n",
    "- By predicting the next token well across billions of examples, the model learns grammar, semantics, style, reasoning patterns, and world knowledge.  \n",
    "- All of this comes from one simple principle: *minimize negative log-likelihood (maximize the probability of real text).*\n",
    "\n",
    "---\n",
    "\n",
    "**Next (Block 1.3):**  \n",
    "We extend this objective to **batches of sequences** (with different lengths), introduce **padding**, and show how to compute a **masked mean token loss** that ignores `<pad>` tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23429d",
   "metadata": {},
   "source": [
    "## 3) Batching, Padding, and the Masked Token Loss\n",
    "\n",
    "So far we looked at single sequences. But in practice, models are trained with **batches** of sequences in parallel, for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Why batching?\n",
    "\n",
    "- GPUs/TPUs can process many examples at once.  \n",
    "- Instead of updating weights after every single sequence, we group **B sequences** into a **batch**.  \n",
    "- This allows **vectorized operations** and faster convergence (by averaging gradients).\n",
    "\n",
    "Formally:  \n",
    "A batch $\\mathcal{B}$ is a set of $B$ token sequences:\n",
    "$$\n",
    "\\mathcal{B} = \\{\\mathbf{t}^{(1)}, \\mathbf{t}^{(2)}, \\dots, \\mathbf{t}^{(B)}\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 The problem of variable lengths\n",
    "\n",
    "Different sequences have different lengths:\n",
    "- Example: `\"The cat\"` → 2 tokens.  \n",
    "- Example: `\"A very long sentence here\"` → 6 tokens.  \n",
    "\n",
    "Neural networks expect tensors with the **same shape**, so we cannot stack them directly.  \n",
    "Solution: **Padding**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Padding sequences\n",
    "\n",
    "Let $L_{\\max}$ be the length of the longest sequence in the batch.  \n",
    "We build a matrix:\n",
    "\n",
    "$$\n",
    "T \\in \\mathbb{N}^{B \\times L_{\\max}}, \\quad\n",
    "T_{b,i} =\n",
    "\\begin{cases}\n",
    "t^{(b)}_i & \\text{if } i \\leq L(\\mathbf{t}^{(b)}), \\\\\\\\\n",
    "\\texttt{<pad>} & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Each row = one sequence.  \n",
    "- Shorter sequences are padded with a special `<pad>` token until they reach length $L_{\\max}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Mask of valid tokens\n",
    "\n",
    "To ensure padding does not affect the loss, we build a **mask**:\n",
    "\n",
    "$$\n",
    "M \\in \\{0,1\\}^{B \\times L_{\\max}}, \\qquad\n",
    "M_{b,i} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } T_{b,i} \\neq \\texttt{<pad>} \\quad (\\text{valid token}), \\\\\\\\\n",
    "0 & \\text{if } T_{b,i} = \\texttt{<pad>} \\quad (\\text{ignored}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Masked mean token loss\n",
    "\n",
    "Without masking, the loss would include `<pad>` tokens, corrupting training.  \n",
    "Instead we average only over **valid tokens**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{token}} =\n",
    "- \\frac{1}{N_{\\text{valid}}}\n",
    "\\sum_{b=1}^B \\sum_{i=1}^{L_{\\max}}\n",
    "M_{b,i} \\; \\log p_\\theta\\!\\big(T_{b,i} \\mid T_{b,<i}\\big),\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "N_{\\text{valid}} = \\sum_{b=1}^B \\sum_{i=1}^{L_{\\max}} M_{b,i}.\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "- Loss is **independent of how much padding there is**.  \n",
    "- Gradients reflect only *real tokens*.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.6 Implementation intuition\n",
    "\n",
    "Most deep learning frameworks implement this using:\n",
    "- A **CrossEntropyLoss** with `ignore_index=pad_id` (in PyTorch).  \n",
    "- An **attention_mask** to indicate valid tokens (in Transformers).\n",
    "\n",
    "Thus:\n",
    "- Forward pass uses the padded tensor.  \n",
    "- Loss computation ignores `<pad>`.  \n",
    "- Attention layers also ignore `<pad>` (via masking).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600f8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([3, 6])\n",
      "Mask:\n",
      " tensor([[1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1]])\n",
      "Targets (same shape as batch):\n",
      " tensor([[ 5,  6,  7,  8,  0,  0],\n",
      "        [ 9, 10, 11,  0,  0,  0],\n",
      "        [12, 13, 14, 15, 16, 17]])\n",
      "Masked loss: 3.303070306777954\n",
      "Unmasked loss (PAD counted): 3.4462995529174805\n",
      "Masked loss   (PAD ignored): 3.303070306777954\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose we have 3 sequences of different lengths\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],      # length 4\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id], # length 3\n",
    "    [12, 13, 14, 15, 16, 17]            # length 6 (max)\n",
    "])\n",
    "\n",
    "print(\"Batch shape:\", batch.shape)  # (3, 6)\n",
    "\n",
    "# Mask: 1 for valid tokens, 0 for pad\n",
    "mask = (batch != pad_id).long()\n",
    "print(\"Mask:\\n\", mask)\n",
    "\n",
    "\n",
    "# Example: \"the cat sleeps\"\n",
    "#\n",
    "# We add special tokens: [BOS] at the start, [EOS] at the end.\n",
    "# Sequence becomes: [BOS], \"the\", \"cat\", \"sleeps\", [EOS]\n",
    "#\n",
    "# The model output (logits) has shape (B, L, V):\n",
    "# - B = batch size (here 1 sentence)\n",
    "# - L = sequence length (5 tokens including BOS/EOS)\n",
    "# - V = vocabulary size\n",
    "#\n",
    "# At each position 'Pos':\n",
    "# - Pos 1: Context = [BOS], model predicts the first real token (\"the\")\n",
    "# - Pos 2: Context = [BOS, \"the\"], model predicts \"cat\"\n",
    "# - Pos 3: Context = [BOS, \"the\", \"cat\"], model predicts \"sleeps\"\n",
    "# - Pos 4: Context = [BOS, \"the\", \"cat\", \"sleeps\"], model predicts [EOS]\n",
    "#\n",
    "# Each row logits[0, Pos, :] is a vector of size V with scores\n",
    "# (before softmax) for all tokens in the vocabulary.\n",
    "# After applying softmax, we get a probability distribution\n",
    "# P(next_token | context).\n",
    "#\n",
    "# In training, we compare this distribution with the true target token\n",
    "# at that position using cross-entropy loss.\n",
    "\n",
    "\n",
    "\n",
    "# Example: logits from a model (random for demo)\n",
    "B, L, V = batch.shape[0], batch.shape[1], 20  # vocab size = 20\n",
    "logits = torch.randn(B, L, V)  # simulation of a model output\n",
    "\n",
    "#Print this line if you want to see logits the number of matrix in will be equal to the number of batches, each row in the matrix is the position of each word, th value in each\n",
    "#row is the probability of that word in the Vocab.\n",
    "#print(\"Logits: \\n\", logits)\n",
    "\n",
    "\n",
    "# Loss function that ignores pad tokens when computing the average.\n",
    "# CrossEntropyLoss expects logits of shape (N, V) and target indices of shape (N,).\n",
    "# 'ignore_index=pad_id' ensures positions labeled as PAD do NOT contribute to the loss.\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "targets = batch.clone()\n",
    "\n",
    "# This line is effectively a no-op for PADs (it re-assigns PAD to PAD).\n",
    "# The intention is to make explicit that PAD tokens remain PAD so that\n",
    "# 'ignore_index' can exclude them from the loss computation.\n",
    "targets[targets == pad_id] = pad_id\n",
    "\n",
    "# A quick debug print to verify the exact target tensor being used.\n",
    "# Good practice: print shapes and a small slice if the tensor is large.\n",
    "print(\"Targets (same shape as batch):\\n\", targets)  # shape: (B, L)\n",
    "\n",
    "# CrossEntropyLoss expects flattened shapes:\n",
    "# - logits.view(-1, V):   (B * L, V)\n",
    "# - targets.view(-1):     (B * L,)\n",
    "# Each row in logits corresponds to exactly one target class index.\n",
    "# Positions where target == pad_id will be ignored in the loss.\n",
    "loss = loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "\n",
    "\n",
    "# .view(-1, V) flattens the first two dimensions (B, L) into one.\n",
    "# The -1 tells PyTorch to automatically infer that size (B*L).\n",
    "# Example: logits.shape = (B, L, V) -> logits.view(-1, V) = (B*L, V).\n",
    "# Each row now corresponds to one token position, with V scores (logits).\n",
    "\n",
    "\n",
    "# Print the scalar loss value. \"Masked\" indicates PAD positions were excluded\n",
    "# thanks to 'ignore_index=pad_id'.\n",
    "print(\"Masked loss:\", loss.item())\n",
    "\n",
    "\n",
    "# Without ignore_index, PAD tokens are treated as real targets:\n",
    "# - Loss & gradients get biased toward predicting PAD\n",
    "# - Shorter sequences contribute many PAD positions\n",
    "# - Metrics (loss/perplexity) become misleading\n",
    "unmasked_loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")  # no ignore_index\n",
    "unmasked_loss = unmasked_loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "\n",
    "print(\"Unmasked loss (PAD counted):\", unmasked_loss.item())\n",
    "print(\"Masked loss   (PAD ignored):\", loss.item())\n",
    "\n",
    "# Rule of thumb:\n",
    "# If your batch contains padding, the unmasked loss is typically larger\n",
    "# and will push the model to predict PAD more often—hurting learning.\n",
    "# Always ignore PAD in the loss (via ignore_index) and in attention masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7eba7d",
   "metadata": {},
   "source": [
    "## 4) Perplexity: measuring how well a model predicts\n",
    "\n",
    "Up to now we focused on the **loss** (negative log-likelihood). But in language modeling, people often report **perplexity (PPL)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Definition\n",
    "\n",
    "For a dataset of $N_{\\text{valid}}$ valid tokens, perplexity is:\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\!\\Bigg( \\frac{1}{N_{\\text{valid}}}\n",
    "\\sum_{n=1}^N \\sum_{i=1}^{L(x^{(n)})}\n",
    "-\\log p_\\theta(t^{(n)}_i \\mid t^{(n)}_{<i}) \\Bigg).\n",
    "$$\n",
    "\n",
    "This is simply the exponential of the **average negative log-likelihood per token**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Intuition\n",
    "\n",
    "- If $\\mathrm{PPL}=1$: the model is perfect — it always assigns probability 1 to the correct token.  \n",
    "- If $\\mathrm{PPL}=V$ (vocabulary size): the model is as bad as random guessing.  \n",
    "- Lower perplexity = better model.\n",
    "\n",
    "In words: **perplexity measures “how many tokens the model is confused among, on average.”**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Why use perplexity?\n",
    "\n",
    "- It is easier to interpret than raw log-loss.  \n",
    "- It is standard in language modeling benchmarks (e.g., Penn Treebank, WikiText).  \n",
    "- It allows fair comparison across models trained with different vocabularies (to some extent).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Example with code\n",
    "Now let’s compute perplexity from the loss in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Suppose we have predictions for a batch\n",
    "pad_id = 0\n",
    "targets = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id],\n",
    "    [12, 13, 14, 15, 16, 17]\n",
    "])\n",
    "\n",
    "B, L = targets.shape\n",
    "V = 20\n",
    "logits = torch.randn(B, L, V)\n",
    "\n",
    "# Loss function that ignores padding\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id, reduction=\"sum\")\n",
    "\n",
    "# Compute total loss (sum over tokens, not mean yet)\n",
    "loss_sum = loss_fn(logits.view(-1, V), targets.view(-1))\n",
    "\n",
    "# Count valid tokens (non-pad)\n",
    "valid_tokens = (targets != pad_id).sum().item()\n",
    "\n",
    "# Average NLL per token\n",
    "nll_avg = loss_sum.item() / valid_tokens\n",
    "\n",
    "# Perplexity\n",
    "ppl = math.exp(nll_avg)\n",
    "\n",
    "print(\"Average NLL per token:\", nll_avg)\n",
    "print(\"Perplexity:\", ppl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6e9b6",
   "metadata": {},
   "source": [
    "## 5) Attention Masking: Causal, Padding, and Head Masks\n",
    "\n",
    "So far we focused on **token probabilities** and **loss functions**.  \n",
    "Now we connect this theory to the **attention mechanism** inside Transformers — the core building block of LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 Self-attention in a nutshell\n",
    "\n",
    "In self-attention, every token can attend to every other token.  \n",
    "For a sequence of length $L$ and embedding size $d$, we compute:\n",
    "\n",
    "- **Queries:** $Q \\in \\mathbb{R}^{L \\times d}$  \n",
    "- **Keys:** $K \\in \\mathbb{R}^{L \\times d}$  \n",
    "- **Values:** $V \\in \\mathbb{R}^{L \\times d}$  \n",
    "\n",
    "The **attention scores** are:\n",
    "\n",
    "$$\n",
    "S = \\frac{QK^\\top}{\\sqrt{d}} \\quad \\in \\mathbb{R}^{L \\times L}.\n",
    "$$\n",
    "\n",
    "After applying softmax row-wise, we get the **attention weights**:\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}(S),\n",
    "$$\n",
    "\n",
    "and then the output is:\n",
    "\n",
    "$$\n",
    "O = A \\, V.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 The problem\n",
    "\n",
    "- Without restrictions, a token at position $i$ could attend to **future tokens** $j>i$.  \n",
    "- `<pad>` tokens could receive attention, contaminating representations.  \n",
    "- Sometimes we want to **disable certain heads** for ablation or interpretability.\n",
    "\n",
    "**Solution:** use **masks** that tell the model where it is allowed to attend.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Causal mask (autoregression)\n",
    "\n",
    "**Goal:** prevent a token from looking into the future.\n",
    "\n",
    "Definition:\n",
    "\n",
    "$$\n",
    "C \\in \\{0,1\\}^{L \\times L}, \\qquad\n",
    "C_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } j>i \\quad (\\text{future blocked}), \\\\\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Applied to scores:\n",
    "\n",
    "$$\n",
    "S \\;\\leftarrow\\; S + (-10^9)\\cdot C.\n",
    "$$\n",
    "\n",
    "This sets future positions to a very large negative value (≈$-\\infty$), so after softmax they get probability 0.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Padding mask\n",
    "\n",
    "**Goal:** ignore `<pad>` tokens added during batching.\n",
    "\n",
    "Definition for a batch of size $B$:\n",
    "\n",
    "$$\n",
    "P \\in \\{0,1\\}^{B \\times L}, \\qquad\n",
    "P_{b,j} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if token $j$ in sequence $b$ is <pad>}, \\\\\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "When broadcasted into attention scores, positions marked with 1 are also masked out.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Head mask\n",
    "\n",
    "**Goal:** enable or disable specific heads.\n",
    "\n",
    "For $H$ attention heads:\n",
    "\n",
    "$$\n",
    "h \\in \\{0,1\\}^H \\quad \\text{(or } h \\in \\{0,1\\}^{B \\times H}\\text{ if per-example)}.\n",
    "$$\n",
    "\n",
    "- $h_h=0$ → that head is disabled (all its contributions zeroed).  \n",
    "- Useful for **interpretability** (probing which heads matter), or for **structured pruning**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.6 Combined masking\n",
    "\n",
    "For a full batch, the final scores are:\n",
    "\n",
    "$$\n",
    "S = \\frac{QK^\\top}{\\sqrt{d}}.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Broadcast $C$ (causal mask) to $(1,1,L,L)$.  \n",
    "2. Broadcast $P$ (padding mask) to $(B,1,1,L)$.  \n",
    "3. Combine:  \n",
    "   $$\n",
    "   M = \\text{broadcast}(C) \\;\\lor\\; \\text{broadcast}(P).\n",
    "   $$\n",
    "4. Apply to scores:  \n",
    "   $$\n",
    "   S \\;\\leftarrow\\; S + (-10^9)\\cdot M.\n",
    "   $$\n",
    "5. Softmax row-wise over keys:  \n",
    "   $$\n",
    "   A = \\text{softmax}(S).\n",
    "   $$\n",
    "6. Apply head mask (if any):  \n",
    "   $$\n",
    "   A \\;\\leftarrow\\; A \\odot \\text{broadcast}(h).\n",
    "   $$\n",
    "7. Final output:  \n",
    "   $$\n",
    "   O = A \\, V.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.7 Why attention masking is essential\n",
    "\n",
    "- **Causal mask:** enforces the autoregressive factorization (no cheating by looking at the future).  \n",
    "- **Padding mask:** ensures that `<pad>` tokens do not leak into the computation.  \n",
    "- **Head mask:** gives flexibility for analysis and pruning.\n",
    "\n",
    "Together, these masks align the **mathematics of training (loss + padding)** with the **architecture of attention**.\n",
    "\n",
    "---\n",
    "\n",
    "**Next (Block 1.6):**  \n",
    "We will implement these masks in **PyTorch code**:  \n",
    "1. Build a causal mask matrix.  \n",
    "2. Build a padding mask from a batch with `<pad>`.  \n",
    "3. Show how they are applied inside a MultiheadAttention layer.  \n",
    "4. Visualize the causal mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ff7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Create toy batch (with padding)\n",
    "# -------------------------------\n",
    "pad_id = 0\n",
    "batch = torch.tensor([\n",
    "    [5, 6, 7, 8, pad_id, pad_id],      # length 4\n",
    "    [9, 10, 11, pad_id, pad_id, pad_id], # length 3\n",
    "    [12, 13, 14, 15, 16, 17]            # length 6 (max)\n",
    "])\n",
    "\n",
    "B, L = batch.shape\n",
    "D = 16   # embedding dimension\n",
    "H = 2    # number of heads\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build causal mask\n",
    "# -------------------------------\n",
    "# Shape: (L, L)\n",
    "causal_mask = torch.triu(torch.ones(L, L) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "print(\"Causal mask:\\n\", causal_mask)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Build key-padding mask\n",
    "# -------------------------------\n",
    "# True where PAD tokens are present\n",
    "padding_mask = (batch == pad_id)\n",
    "print(\"Padding mask shape:\", padding_mask.shape)  # (B, L)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. MultiheadAttention with masks\n",
    "# -------------------------------\n",
    "attn = nn.MultiheadAttention(embed_dim=D, num_heads=H, batch_first=True)\n",
    "\n",
    "# Random embeddings for the tokens\n",
    "x = torch.randn(B, L, D)\n",
    "\n",
    "# Apply attention with both masks\n",
    "out, attn_weights = attn(\n",
    "    x, x, x,\n",
    "    attn_mask=causal_mask,         # (L, L)\n",
    "    key_padding_mask=padding_mask  # (B, L)\n",
    ")\n",
    "\n",
    "print(\"Output shape:\", out.shape)                 # (B, L, D)\n",
    "print(\"Attention weights shape:\", attn_weights.shape) # (B*H, L, L)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Visualize causal mask\n",
    "# -------------------------------\n",
    "plt.imshow(causal_mask.numpy(), cmap=\"gray\")\n",
    "plt.title(\"Causal Attention Mask (white = -inf)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aeed6e",
   "metadata": {},
   "source": [
    "## 6) Attention Masking Implementation: Mathematical Foundations and PyTorch Realization\n",
    "\n",
    "In this section we bridge the **mathematical theory** of autoregressive language modeling with the **architectural constraints** enforced by attention masking. We will prove that proper masking is not just an implementation detail, but a **necessary condition** for the mathematical coherence of the training objective.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1 Mathematical preliminaries: Attention as conditional probability computation\n",
    "\n",
    "**Formal setup:**  \n",
    "Let $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$ be a sequence of token embeddings, where $L$ is sequence length and $d$ is embedding dimension.\n",
    "\n",
    "The **scaled dot-product attention** mechanism computes:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q = XW_Q \\in \\mathbb{R}^{L \\times d_k}$ (queries)\n",
    "- $K = XW_K \\in \\mathbb{R}^{L \\times d_k}$ (keys)  \n",
    "- $V = XW_V \\in \\mathbb{R}^{L \\times d_v}$ (values)\n",
    "- $W_Q, W_K \\in \\mathbb{R}^{d \\times d_k}$, $W_V \\in \\mathbb{R}^{d \\times d_v}$ are learned parameters\n",
    "\n",
    "**Critical observation:**  \n",
    "The attention weights $A_{ij} = \\text{softmax}_{j}\\left(\\frac{q_i^T k_j}{\\sqrt{d_k}}\\right)$ represent the **probability** that query $i$ attends to key $j$.\n",
    "\n",
    "**Mathematical constraint from autoregression:**  \n",
    "For the model to respect the causal factorization $p_\\theta(t_i | t_{<i})$, we must enforce:\n",
    "\n",
    "$$\n",
    "A_{ij} = 0 \\quad \\forall j > i\n",
    "$$\n",
    "\n",
    "This is not optional—it is a **mathematical necessity** derived from the independence assumptions in autoregressive modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Formal definition of attention masks\n",
    "\n",
    "**Definition 6.1 (Attention Mask):**  \n",
    "An attention mask is a function $M: \\mathbb{N} \\times \\mathbb{N} \\rightarrow \\{0, -\\infty\\}$ that modifies attention scores:\n",
    "\n",
    "$$\n",
    "\\tilde{S}_{ij} = S_{ij} + M(i,j)\n",
    "$$\n",
    "\n",
    "where $S_{ij} = \\frac{q_i^T k_j}{\\sqrt{d_k}}$ are the raw attention scores.\n",
    "\n",
    "**Definition 6.2 (Causal Mask):**  \n",
    "The causal mask $C: \\mathbb{N} \\times \\mathbb{N} \\rightarrow \\{0, -\\infty\\}$ is defined as:\n",
    "\n",
    "$$\n",
    "C(i,j) = \\begin{cases}\n",
    "0 & \\text{if } j \\leq i \\\\\n",
    "-\\infty & \\text{if } j > i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Theorem 6.1 (Causal Consistency):**  \n",
    "*The causal mask is necessary and sufficient to ensure that the attention mechanism respects autoregressive ordering.*\n",
    "\n",
    "**Proof:**  \n",
    "*Necessity:* Suppose $C(i,j) \\neq -\\infty$ for some $j > i$. Then after softmax, $A_{ij} > 0$, meaning position $i$ can access information from future position $j$, violating the conditional independence $p(t_i | t_{<i})$.\n",
    "\n",
    "*Sufficiency:* If $C(i,j) = -\\infty$ for all $j > i$, then $\\tilde{S}_{ij} = -\\infty$, so $A_{ij} = 0$ after softmax, ensuring position $i$ only accesses $\\{t_1, \\ldots, t_i\\}$. □\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Padding mask: Mathematical treatment of variable-length sequences\n",
    "\n",
    "**Problem statement:**  \n",
    "In batched training, sequences have different lengths $L_1, L_2, \\ldots, L_B$. To form tensors, we pad to $L_{\\max} = \\max_b L_b$ using a special token $\\text{<pad>}$.\n",
    "\n",
    "**Mathematical issue:**  \n",
    "Padded positions contain **no semantic information** but standard attention would still compute:\n",
    "\n",
    "$$\n",
    "o_i = \\sum_{j=1}^{L_{\\max}} A_{ij} v_j = \\sum_{j=1}^{L_b} A_{ij} v_j + \\sum_{j=L_b+1}^{L_{\\max}} A_{ij} v_{\\text{<pad>}}\n",
    "$$\n",
    "\n",
    "The second sum **contaminates** the representation with meaningless pad embeddings.\n",
    "\n",
    "**Definition 6.3 (Key-Padding Mask):**  \n",
    "For a batch $\\mathcal{B} = \\{\\mathbf{t}^{(b)}\\}_{b=1}^B$ with lengths $\\{L_b\\}$, the key-padding mask is:\n",
    "\n",
    "$$\n",
    "P^{(b)}(i,j) = \\begin{cases}\n",
    "0 & \\text{if } j \\leq L_b \\\\\n",
    "-\\infty & \\text{if } j > L_b\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Theorem 6.2 (Padding Invariance):**  \n",
    "*With proper key-padding masking, the attention output is invariant to the amount of padding.*\n",
    "\n",
    "**Proof:**  \n",
    "Let $\\tilde{S}^{(b)}_{ij} = S^{(b)}_{ij} + P^{(b)}(i,j)$. For $j > L_b$, we have $\\tilde{S}^{(b)}_{ij} = -\\infty$, so:\n",
    "\n",
    "$$\n",
    "A^{(b)}_{ij} = \\frac{\\exp(\\tilde{S}^{(b)}_{ij})}{\\sum_{k=1}^{L_{\\max}} \\exp(\\tilde{S}^{(b)}_{ik})} = \\frac{0}{\\sum_{k=1}^{L_b} \\exp(\\tilde{S}^{(b)}_{ik})} = 0\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "o_i^{(b)} = \\sum_{j=1}^{L_b} A^{(b)}_{ij} v_j^{(b)}\n",
    "$$\n",
    "\n",
    "which depends only on the first $L_b$ positions, regardless of $L_{\\max}$. □"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a25add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define special tokens with clear semantic meaning\n",
    "PAD_ID = 0   # Padding token (no semantic content)\n",
    "BOS_ID = 1   # Beginning of sequence marker\n",
    "EOS_ID = 2   # End of sequence marker\n",
    "\n",
    "# Create a realistic batch demonstrating the masking problem\n",
    "# Each sequence represents: [BOS, content_tokens..., EOS, PAD, PAD, ...]\n",
    "batch_tokens = torch.tensor([\n",
    "    [BOS_ID, 5, 6, 7, 8, EOS_ID, PAD_ID, PAD_ID],        # Real length: 6\n",
    "    [BOS_ID, 9, 10, 11, EOS_ID, PAD_ID, PAD_ID, PAD_ID],  # Real length: 5  \n",
    "    [BOS_ID, 12, 13, 14, 15, 16, 17, EOS_ID]             # Real length: 8 (no padding)\n",
    "])\n",
    "\n",
    "B, L = batch_tokens.shape  # B=3 sequences, L=8 max length\n",
    "print(f\"Batch shape: {batch_tokens.shape}\")\n",
    "print(\"Batch contents (showing real vs padded tokens):\")\n",
    "for i, seq in enumerate(batch_tokens):\n",
    "    real_tokens = seq[seq != PAD_ID]\n",
    "    pad_count = (seq == PAD_ID).sum().item()\n",
    "    print(f\"  Sequence {i}: {real_tokens.tolist()} + {pad_count} padding tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b79c06",
   "metadata": {},
   "source": [
    "### 6.4 Constructing the causal mask: Theory and implementation\n",
    "\n",
    "**Mathematical derivation:**  \n",
    "We need to construct a mask matrix $C \\in \\{0, -\\infty\\}^{L \\times L}$ such that:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\begin{cases}\n",
    "0 & \\text{if } j \\leq i \\text{ (causally valid)} \\\\\n",
    "-\\infty & \\text{if } j > i \\text{ (causally invalid)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This can be expressed using the **upper triangular matrix** with offset:\n",
    "\n",
    "$$\n",
    "C = -\\infty \\cdot \\text{triu}(\\mathbf{1}_{L \\times L}, k=1)\n",
    "$$\n",
    "\n",
    "where $\\text{triu}(\\cdot, k=1)$ extracts the strictly upper triangular part.\n",
    "\n",
    "**Computational complexity:**  \n",
    "The causal mask is sequence-length dependent: $O(L^2)$ space and $O(L^2)$ time to construct. For very long sequences (e.g., $L = 100k$), this becomes significant.\n",
    "\n",
    "**Implementation note:**  \n",
    "In practice, the mask is pre-computed once per sequence length and reused across batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_length, device=None, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Construct a causal attention mask preventing future information leakage.\n",
    "    \n",
    "    Mathematical definition:\n",
    "        C[i,j] = 0     if j <= i (position j is causally valid for query i)\n",
    "        C[i,j] = -inf  if j > i  (position j is in the future relative to i)\n",
    "    \n",
    "    Args:\n",
    "        seq_length (int): Length of the sequence\n",
    "        device (torch.device): Device to place the mask on\n",
    "        dtype (torch.dtype): Data type for the mask\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Causal mask of shape (seq_length, seq_length)\n",
    "        \n",
    "    Time complexity: O(L²)\n",
    "    Space complexity: O(L²)\n",
    "    \"\"\"\n",
    "    # Method 1: Using triu (upper triangular)\n",
    "    # Create matrix of ones, extract upper triangle starting from diagonal+1\n",
    "    upper_triangle = torch.triu(torch.ones(seq_length, seq_length, dtype=dtype), diagonal=1)\n",
    "    \n",
    "    # Convert 1s to -inf (blocked positions) and 0s remain 0 (allowed positions)\n",
    "    causal_mask = upper_triangle.masked_fill(upper_triangle == 1, float('-inf'))\n",
    "    \n",
    "    if device is not None:\n",
    "        causal_mask = causal_mask.to(device)\n",
    "    \n",
    "    return causal_mask\n",
    "\n",
    "def verify_causal_mask_properties(mask):\n",
    "    \"\"\"\n",
    "    Verify mathematical properties of the causal mask.\n",
    "    \"\"\"\n",
    "    L = mask.shape[0]\n",
    "    print(f\"=== Causal Mask Verification (L={L}) ===\")\n",
    "    \n",
    "    # Property 1: Lower triangular + diagonal should be 0\n",
    "    lower_triangular = torch.tril(mask)\n",
    "    if torch.all(lower_triangular == 0):\n",
    "        print(\"✅ Lower triangular part is zero (causally valid positions)\")\n",
    "    else:\n",
    "        print(\"❌ Lower triangular part contains non-zero values\")\n",
    "    \n",
    "    # Property 2: Upper triangular should be -inf\n",
    "    upper_triangular = torch.triu(mask, diagonal=1)\n",
    "    if torch.all(upper_triangular == float('-inf')):\n",
    "        print(\"✅ Upper triangular part is -inf (future positions blocked)\")\n",
    "    else:\n",
    "        print(\"❌ Upper triangular part contains finite values\")\n",
    "    \n",
    "    # Property 3: Diagonal should be 0 (token can attend to itself)\n",
    "    diagonal = torch.diag(mask)\n",
    "    if torch.all(diagonal == 0):\n",
    "        print(\"✅ Diagonal is zero (self-attention allowed)\")\n",
    "    else:\n",
    "        print(\"❌ Diagonal contains non-zero values\")\n",
    "\n",
    "# Create and verify causal mask\n",
    "causal_mask = create_causal_mask(L)\n",
    "print(f\"Causal mask shape: {causal_mask.shape}\")\n",
    "print(\"Causal mask (showing first 5×5 submatrix):\")\n",
    "print(causal_mask[:5, :5])\n",
    "\n",
    "verify_causal_mask_properties(causal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077602f",
   "metadata": {},
   "source": [
    "### 6.5 Key-padding mask: Measure-theoretic foundations\n",
    "\n",
    "**Problem formalization:**  \n",
    "In measure theory, padding introduces **zero-measure support** in the token space. Let $\\mathcal{T}$ be the space of valid tokens and $\\mu$ be the natural counting measure. Padding tokens live in $\\mathcal{T}_{\\text{pad}} = \\{\\text{<pad>}\\}$ where $\\mu(\\mathcal{T}_{\\text{pad}}) = 0$ semantically.\n",
    "\n",
    "**Definition 6.4 (Semantic Support):**  \n",
    "For a padded sequence $\\tilde{\\mathbf{t}} = (t_1, \\ldots, t_{L_{\\text{real}}}, \\text{<pad>}, \\ldots, \\text{<pad>})$, the semantic support is:\n",
    "\n",
    "$$\n",
    "\\text{supp}_{\\text{sem}}(\\tilde{\\mathbf{t}}) = \\{i : \\tilde{t}_i \\neq \\text{<pad>}\\} = \\{1, 2, \\ldots, L_{\\text{real}}\\}\n",
    "$$\n",
    "\n",
    "**Theorem 6.3 (Attention Concentration):**  \n",
    "*For any query position $i$, attention weights must satisfy the concentration property:*\n",
    "\n",
    "$$\n",
    "\\sum_{j \\in \\text{supp}_{\\text{sem}}(\\tilde{\\mathbf{t}})} A_{ij} = 1, \\quad A_{ij} = 0 \\text{ for } j \\notin \\text{supp}_{\\text{sem}}(\\tilde{\\mathbf{t}})\n",
    "$$\n",
    "\n",
    "**Proof sketch:**  \n",
    "This follows from the normalization constraint of softmax and the requirement that $\\exp(-\\infty) = 0$.\n",
    "\n",
    "**Broadcasting theory:**  \n",
    "When working with batches, the key-padding mask must be broadcast correctly. Let $\\mathcal{B} = \\{\\mathbf{t}^{(b)}\\}_{b=1}^B$ with support sets $\\{S_b\\}_{b=1}^B$ where $S_b = \\text{supp}_{\\text{sem}}(\\mathbf{t}^{(b)})$.\n",
    "\n",
    "The batch padding mask $\\mathbf{P} \\in \\{0, -\\infty\\}^{B \\times L_{\\max}}$ satisfies:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{b,j} = \\begin{cases}\n",
    "0 & \\text{if } j \\in S_b \\\\\n",
    "-\\infty & \\text{if } j \\notin S_b\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Computational considerations:**  \n",
    "- **Space complexity:** $O(B \\times L_{\\max})$ for the mask tensor\n",
    "- **Broadcasting complexity:** Converting from $(B, L)$ to $(B, H, L, L)$ requires $O(B \\times H \\times L^2)$ memory\n",
    "- **Cache efficiency:** Masks are typically sparse and benefit from specialized sparse attention kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0940edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(batch_tokens, pad_id, return_inverse=False):\n",
    "    \"\"\"\n",
    "    Create key-padding mask with comprehensive mathematical verification.\n",
    "    \n",
    "    Mathematical definition:\n",
    "        For batch element b and position j:\n",
    "        P[b,j] = True   if token[b,j] == pad_id (position to ignore)\n",
    "        P[b,j] = False  if token[b,j] != pad_id (valid position)\n",
    "    \n",
    "    Args:\n",
    "        batch_tokens (torch.Tensor): Shape (B, L) containing token IDs\n",
    "        pad_id (int): Token ID used for padding\n",
    "        return_inverse (bool): If True, also return the inverse mask\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask, True for positions to ignore\n",
    "        torch.Tensor (optional): Inverse mask for convenience\n",
    "    \"\"\"\n",
    "    # Create boolean mask: True where padding tokens exist\n",
    "    padding_mask = (batch_tokens == pad_id)\n",
    "    \n",
    "    if return_inverse:\n",
    "        # Inverse mask: True for valid (non-padding) positions\n",
    "        valid_mask = ~padding_mask\n",
    "        return padding_mask, valid_mask\n",
    "    \n",
    "    return padding_mask\n",
    "\n",
    "def analyze_batch_padding_statistics(batch_tokens, pad_id):\n",
    "    \"\"\"\n",
    "    Analyze padding patterns and their impact on computational efficiency.\n",
    "    \"\"\"\n",
    "    B, L = batch_tokens.shape\n",
    "    \n",
    "    # Compute sequence lengths\n",
    "    sequence_lengths = []\n",
    "    for b in range(B):\n",
    "        seq_len = (batch_tokens[b] != pad_id).sum().item()\n",
    "        sequence_lengths.append(seq_len)\n",
    "    \n",
    "    # Padding statistics\n",
    "    total_tokens = B * L\n",
    "    valid_tokens = sum(sequence_lengths)\n",
    "    padding_tokens = total_tokens - valid_tokens\n",
    "    padding_ratio = padding_tokens / total_tokens\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    avg_length = np.mean(sequence_lengths)\n",
    "    length_variance = np.var(sequence_lengths)\n",
    "    efficiency = valid_tokens / total_tokens  # Fraction of useful computation\n",
    "    \n",
    "    print(f\"=== Batch Padding Analysis ===\")\n",
    "    print(f\"Batch size: {B}, Max length: {L}\")\n",
    "    print(f\"Sequence lengths: {sequence_lengths}\")\n",
    "    print(f\"Average length: {avg_length:.2f} ± {np.sqrt(length_variance):.2f}\")\n",
    "    print(f\"Valid tokens: {valid_tokens}/{total_tokens} ({efficiency:.1%})\")\n",
    "    print(f\"Padding ratio: {padding_ratio:.1%}\")\n",
    "    print(f\"Computational efficiency: {efficiency:.1%}\")\n",
    "    \n",
    "    # Memory overhead analysis\n",
    "    mask_memory = B * L  # Boolean mask\n",
    "    attention_memory = B * L * L  # Full attention matrix\n",
    "    print(f\"Mask memory overhead: {mask_memory} bools\")\n",
    "    print(f\"Attention memory: {attention_memory} floats\")\n",
    "\n",
    "# Create padding mask and analyze batch\n",
    "key_padding_mask, valid_mask = create_key_padding_mask(batch_tokens, PAD_ID, return_inverse=True)\n",
    "\n",
    "print(\"Key padding mask (True = ignore, False = attend):\")\n",
    "print(key_padding_mask.int())\n",
    "print(\"\\nValid token mask (True = attend, False = ignore):\")\n",
    "print(valid_mask.int())\n",
    "\n",
    "analyze_batch_padding_statistics(batch_tokens, PAD_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16f85f",
   "metadata": {},
   "source": [
    "### 6.6 Mask composition theory: Linear algebra of attention constraints\n",
    "\n",
    "**Composition operator:**  \n",
    "Multiple masks must be combined via the **Hadamard composition** in log-space. Given masks $M_1, M_2, \\ldots, M_k$, the composed mask is:\n",
    "\n",
    "$$\n",
    "M_{\\text{composed}} = M_1 \\oplus M_2 \\oplus \\cdots \\oplus M_k\n",
    "$$\n",
    "\n",
    "where $\\oplus$ is the **logical OR** operation in $\\{0, -\\infty\\}$ space:\n",
    "\n",
    "$$\n",
    "a \\oplus b = \\begin{cases}\n",
    "0 & \\text{if } a = 0 \\text{ AND } b = 0 \\\\\n",
    "-\\infty & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Theorem 6.4 (Mask Commutativity):**  \n",
    "*The mask composition operator is commutative and associative:*\n",
    "\n",
    "$$\n",
    "M_1 \\oplus M_2 = M_2 \\oplus M_1, \\quad (M_1 \\oplus M_2) \\oplus M_3 = M_1 \\oplus (M_2 \\oplus M_3)\n",
    "$$\n",
    "\n",
    "**Broadcasting mathematics:**  \n",
    "Consider the dimensional analysis:\n",
    "- Causal mask: $C \\in \\{0, -\\infty\\}^{L \\times L}$\n",
    "- Padding mask: $P \\in \\{0, -\\infty\\}^{B \\times L}$  \n",
    "- Attention scores: $S \\in \\mathbb{R}^{B \\times H \\times L \\times L}$\n",
    "\n",
    "**Broadcasting transformation:**\n",
    "$$\n",
    "\\begin{align}\n",
    "C_{\\text{broadcast}} &: (L, L) \\to (1, 1, L, L) \\to (B, H, L, L) \\\\\n",
    "P_{\\text{broadcast}} &: (B, L) \\to (B, 1, 1, L) \\to (B, H, L, L)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Final mask application:**\n",
    "$$\n",
    "\\tilde{S}_{bhij} = S_{bhij} + C_{\\text{broadcast}}[b,h,i,j] + P_{\\text{broadcast}}[b,h,i,j]\n",
    "$$\n",
    "\n",
    "**Theorem 6.5 (Attention Conservation):**  \n",
    "*Under proper masking, attention weights satisfy:*\n",
    "\n",
    "$$\n",
    "\\sum_{j \\in \\mathcal{V}_{bi}} A_{bhij} = 1 \\quad \\forall b,h,i\n",
    "$$\n",
    "\n",
    "*where $\\mathcal{V}_{bi} = \\{j : j \\leq i \\text{ AND } \\mathbf{t}^{(b)}_j \\neq \\text{<pad>}\\}$ is the valid attention set.*\n",
    "\n",
    "**Proof:**  \n",
    "The softmax normalization over the unmasked set ensures unit mass distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attention_masks(scores, causal_mask=None, key_padding_mask=None, head_mask=None):\n",
    "    \"\"\"\n",
    "    Apply multiple attention masks with rigorous dimensional analysis.\n",
    "    \n",
    "    Mathematical operation:\n",
    "        scores_masked = scores + causal_mask_broadcasted + padding_mask_broadcasted\n",
    "    \n",
    "    Args:\n",
    "        scores (torch.Tensor): Shape (B, H, L, L) - raw attention scores\n",
    "        causal_mask (torch.Tensor): Shape (L, L) - causal constraints\n",
    "        key_padding_mask (torch.Tensor): Shape (B, L) - padding constraints  \n",
    "        head_mask (torch.Tensor): Shape (H,) - head enable/disable\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Masked scores ready for softmax\n",
    "        dict: Broadcasting statistics and verification info\n",
    "    \"\"\"\n",
    "    B, H, L_q, L_k = scores.shape\n",
    "    original_scores = scores.clone()\n",
    "    \n",
    "    # Verification info\n",
    "    mask_info = {\n",
    "        'original_shape': scores.shape,\n",
    "        'masks_applied': [],\n",
    "        'broadcasting_ops': []\n",
    "    }\n",
    "    \n",
    "    # Apply causal mask\n",
    "    if causal_mask is not None:\n",
    "        assert causal_mask.shape == (L_q, L_k), f\"Causal mask shape {causal_mask.shape} != ({L_q}, {L_k})\"\n",
    "        \n",
    "        # Broadcasting: (L, L) -> (1, 1, L, L) -> (B, H, L, L)\n",
    "        causal_broadcasted = causal_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, L, L)\n",
    "        scores = scores + causal_broadcasted\n",
    "        \n",
    "        mask_info['masks_applied'].append('causal')\n",
    "        mask_info['broadcasting_ops'].append(f\"causal: {causal_mask.shape} -> {causal_broadcasted.shape}\")\n",
    "    \n",
    "    # Apply key-padding mask\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.shape == (B, L_k), f\"Padding mask shape {key_padding_mask.shape} != ({B}, {L_k})\"\n",
    "        \n",
    "        # Broadcasting: (B, L) -> (B, 1, 1, L) -> (B, H, L, L)\n",
    "        # We expand over the query dimension (dim=2) since we mask keys\n",
    "        padding_broadcasted = key_padding_mask.unsqueeze(1).unsqueeze(1)  # (B, 1, 1, L)\n",
    "        \n",
    "        # Convert boolean mask to additive mask: True -> -inf, False -> 0\n",
    "        padding_additive = torch.where(padding_broadcasted, \n",
    "                                     torch.tensor(float('-inf'), dtype=scores.dtype, device=scores.device),\n",
    "                                     torch.tensor(0.0, dtype=scores.dtype, device=scores.device))\n",
    "        \n",
    "        scores = scores + padding_additive\n",
    "        \n",
    "        mask_info['masks_applied'].append('key_padding')\n",
    "        mask_info['broadcasting_ops'].append(f\"padding: {key_padding_mask.shape} -> {padding_additive.shape}\")\n",
    "    \n",
    "    # Apply head mask (after softmax, so we'll return it for later application)\n",
    "    if head_mask is not None:\n",
    "        assert head_mask.shape == (H,), f\"Head mask shape {head_mask.shape} != ({H},)\"\n",
    "        mask_info['masks_applied'].append('head')\n",
    "        mask_info['head_mask'] = head_mask\n",
    "    \n",
    "    return scores, mask_info\n",
    "\n",
    "def verify_mask_application(original_scores, masked_scores, mask_info):\n",
    "    \"\"\"\n",
    "    Verify that mask application preserves mathematical properties.\n",
    "    \"\"\"\n",
    "    print(\"=== Mask Application Verification ===\")\n",
    "    \n",
    "    # Check that finite values are preserved where no mask is applied\n",
    "    finite_original = torch.isfinite(original_scores)\n",
    "    finite_masked = torch.isfinite(masked_scores)\n",
    "    \n",
    "    print(f\"Original finite values: {finite_original.sum().item()}\")\n",
    "    print(f\"Masked finite values: {finite_masked.sum().item()}\")\n",
    "    print(f\"Masks applied: {mask_info['masks_applied']}\")\n",
    "    print(f\"Broadcasting operations: {mask_info['broadcasting_ops']}\")\n",
    "    \n",
    "    # Verify that -inf values are introduced only where intended\n",
    "    new_neginf = torch.isneginf(masked_scores) & ~torch.isneginf(original_scores)\n",
    "    print(f\"New -inf positions introduced: {new_neginf.sum().item()}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Demonstrate mask application\n",
    "B, H, L = batch_tokens.shape[0], 2, batch_tokens.shape[1]\n",
    "\n",
    "# Create dummy attention scores (small values for numerical stability)\n",
    "demo_scores = torch.randn(B, H, L, L) * 0.1\n",
    "\n",
    "print(\"Original scores (batch 0, head 0, first 5×5):\")\n",
    "print(demo_scores[0, 0, :5, :5])\n",
    "\n",
    "# Apply both causal and padding masks\n",
    "masked_scores, mask_info = apply_attention_masks(\n",
    "    demo_scores, \n",
    "    causal_mask=causal_mask, \n",
    "    key_padding_mask=key_padding_mask\n",
    ")\n",
    "\n",
    "print(\"\\nAfter applying causal + padding masks:\")\n",
    "print(masked_scores[0, 0, :5, :5])\n",
    "\n",
    "# Verify the application\n",
    "verify_mask_application(demo_scores, masked_scores, mask_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
